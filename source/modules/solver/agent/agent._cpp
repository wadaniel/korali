#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <chrono>

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  //  Pre-allocating space for the experience replay memory
  _experienceReplay.resize(_experienceReplayMaximumSize);

  /*********************************************************************
   *   // If initial generation, set initial agent configuration
   *********************************************************************/

  if (_k->_currentGeneration == 0)
  {
    _randomActionProbabilityCurrentValue = _randomActionProbabilityInitialValue;
    _currentEpisode = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;
    _cumulativeQStar = 0;
    _experienceCount = 0;
    _experienceReplayMaxPriority = 0.0;
    _randomActionProbabilityEffectiveValue = 1.0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestTrainingEpisode = 0;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing profiling timers
  _inputWriteTime = 0.0;
  _outputReadTime = 0.0;
  _normalizationTime = 0.0;
  _forwardStreamTime = 0.0;
  _backwardStreamTime = 0.0;
  _agentStartTime = 0.0;
  _agentWaitTime = 0.0;
}

void Agent::runGeneration()
{
  // Broadcasting initial hyperparameters for all workers to use
  if (_k->_currentGeneration == 1) KORALI_UPDATE_GLOBALS("Hyperparameters", _hyperparameters);

  // Configuring current agent
  Sample agent;
  agent["Sample Id"] = _currentSampleID++;
  agent["Module"] = "Problem";
  agent["Operation"] = "Run Episode";
  agent["Mode"] = "Training";
  agent["Random Action Probability"] = _randomActionProbabilityEffectiveValue;

  // Initializing statistical information
  _currentReward = 0.0;

  // Storage to keep experience information
  knlohmann::json experience;

  // Storage to determine terminal state
  bool isTerminal = false;

  // Launching agent
  auto beginTime = std::chrono::steady_clock::now(); // Profiling
  KORALI_START(agent);
  auto endTime = std::chrono::steady_clock::now(); // Profiling
  _agentStartTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

  // Creating storage for the entire episode before adding it to the replay memory
  std::vector<knlohmann::json> episode;

  // Keep listening to incoming experiences until we reached a terminal state
  while (isTerminal == false)
  {
    // Listening for incoming experiences
    auto beginTime = std::chrono::steady_clock::now(); // Profiling
    experience = KORALI_RECV_MSG_FROM_SAMPLE(agent);
    auto endTime = std::chrono::steady_clock::now(); // Profiling
    _agentWaitTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

    // Storing experience in the episode
    episode.push_back(experience);

    // Increasing total experience counter
    _experienceCount++;

    // Check if experience is terminal
    isTerminal = experience["Termination"] != "Non Terminal";

    // Running Agent training if the following conditions are met
    if (_experienceCount % _experiencesBetweenAgentTrainings == 0)     // If we accumulated enough experiences between updates, update now
      if (_experienceReplay.size() >= _experienceReplayStartSize) // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
        for (size_t i = 0; i < _optimizationStepsPerUpdate; i++)
        {
          trainAgent();
          _optimizationStepCount++;
        }

    // If agent requested new policy, send the new hyperparameters
    if (experience["Request New Policy"] == true)
     KORALI_SEND_MSG_TO_SAMPLE(agent, _hyperparameters);
  }

  // Now that we have the entire episode, process its experiences (add them to replay memory)
  for (size_t i = 0; i < episode.size(); i++) processExperience(episode[i]);

  // Waiting for agent to finish
  KORALI_WAIT(agent);

  // Updating if exceeded best training policy so far.
  if (_currentReward > _bestTrainingReward)
  {
   _bestTrainingReward = _currentReward;
   _bestTrainingEpisode = _currentEpisode;
  }

  // Adding to the number of episodes
  _currentEpisode++;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_currentReward >= _trainingRewardThreshold)
  {
    // Broadcasting hyperparameters testing
    KORALI_UPDATE_GLOBALS("Hyperparameters", _hyperparameters);

    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> agents(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      agents[i]["Sample Id"] = _currentSampleID++;
      agents[i]["Module"] = "Problem";
      agents[i]["Action Source"] = "Policy";
      agents[i]["Operation"] = "Run Episode";
      agents[i]["Mode"] = "Testing";
      agents[i]["Random Action Probability"] = 0.0;

      // Launching agent initialization
      KORALI_START(agents[i]);
    }

    KORALI_WAITALL(agents);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
      _averageTestingReward += agents[i]["Cumulative Reward"].get<float>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;

    // If the average testing reward is better than the previous best, replace it
    // and store hyperparameters as best so far.
    if (_averageTestingReward > _bestAverageTestingReward)
    {
     _bestAverageTestingReward = _averageTestingReward;
     _bestHyperparameters = _hyperparameters;
    }
  }

  /////////////// Updating experience probabilities

  if (_miniBatchStrategy == "Prioritized") updateExperienceReplayProbabilities();

  /////////////// Updating the probability of a random action (epsilon)

  updateRandomActionProbability();
}

void Agent::processExperience(knlohmann::json &experience)
{
  experience_t e;

  e.cache.setMaxAge(_cachePersistence);
  e.cache.setTimer(&_optimizationStepCount);

  // Getting state, action, reward, and whether it's terminal
  e.state = experience["State"].get<std::vector<float>>();
  e.action = experience["Action"].get<std::vector<float>>();
  e.reward = experience["Reward"].get<float>();

  if (experience["Termination"] == "Non Terminal") e.termination = e_nonTerminal;
  if (experience["Termination"] == "Terminal") e.termination = e_terminal;
  if (experience["Termination"] == "Truncated") e.termination = e_truncated;

  e.priority = _experienceReplayMaxPriority;
  e.probability = 0.0;
  e.metadata = experience["Metadata"];
  _experienceReplay.add(e);

  // Adding experience's reward to episode's reward
  _currentReward += e.reward;
}

void Agent::updateExperienceReplayProbabilities()
{
  // Rank priorities
  auto experienceRanking = std::vector<std::pair<float, size_t>>(_experienceReplay.size());
  for (size_t i = 0; i < _experienceReplay.size(); ++i) experienceRanking[i] = std::make_pair(_experienceReplay[i].priority, i + 1);
  sort(experienceRanking.begin(), experienceRanking.end());

  // Calculate rank based probabilities
  float prioritySum = 0.0;
  for (size_t i = 1; i < _experienceReplay.size() + 1; ++i)
    prioritySum += std::pow((float)i, _priorityAnnealingRate);

  // Update probabilities
  for (size_t i = 0; i < _experienceReplay.size(); ++i)
    _experienceReplay[i].probability = std::pow((float)experienceRanking[i].second, _priorityAnnealingRate) / (prioritySum);
}

void Agent::updateExperienceReplayPriority(size_t expId, float priority)
{
  // Insert priority calculated in learner
  if (priority > _experienceReplayMaxPriority) _experienceReplayMaxPriority = priority;
  _experienceReplay[expId].priority = priority;
}

std::vector<float> Agent::calculateImportanceWeights(std::vector<size_t> miniBatch)
{
  auto importanceWeights = std::vector<float>(miniBatch.size(), 1.0);

  if (_miniBatchStrategy == "Prioritized")
  {
    for (size_t i = 0; i < miniBatch.size(); ++i)
      importanceWeights[i] = std::pow((float)_experienceReplay.size() * _experienceReplay[miniBatch[i]].probability, -_importanceWeightAnnealingRate);
  }

  return importanceWeights;
}

std::vector<size_t> Agent::generateMiniBatch(size_t size)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(size);

  if (_miniBatchStrategy == "Uniform")
  {
    for (size_t i = 0; i < size; i++)
    {
      // Producing random (uniform) number for the selection of the experience
      float x = _uniformGenerator->getRandomNumber();

      // Selecting experience
      size_t expId = std::floor(x * (float)(_experienceReplay.size() - 1));

      // Setting experience
      miniBatch[i] = expId;
    }
  }

  if (_miniBatchStrategy == "Prioritized")
  {
    // Allocating storage for uniform random numbers
    auto pvalues = std::vector<float>(size);

    for (size_t i = 0; i < size; i++)
      pvalues[i] = _uniformGenerator->getRandomNumber();

    // Sort selections ascending for traversal
    std::sort(pvalues.begin(), pvalues.end());

    size_t expId = 0;
    size_t batchIdx = 0;
    float cumulativeProbability = 0;

    // Sampling from multinomial distribution using inverse transform
    cumulativeProbability += _experienceReplay[0].probability;
    while (batchIdx < size && expId < _experienceReplay.size())
    {
      if (cumulativeProbability > pvalues[batchIdx])
        miniBatch[batchIdx++] = expId;
      else
        cumulativeProbability += _experienceReplay[++expId].probability;
    }

    // Uniform fill if miniBatch not full due to numerical precision during probability accumulation (cumulative probability < 1)
    while (batchIdx < size)
     miniBatch[batchIdx++] = std::floor(_uniformGenerator->getRandomNumber() * (float)(_experienceReplay.size() - 1));
  }

  return miniBatch;
}

void Agent::updateRandomActionProbability()
{
  // If we reached the starting size
  if (_experienceReplay.size() > _experienceReplayStartSize)
  {
    // Decreasing the value of epsilon
    _randomActionProbabilityCurrentValue = _randomActionProbabilityCurrentValue - _randomActionProbabilityDecreaseRate;

    // Without exceeding the lower bound
    if (_randomActionProbabilityCurrentValue < _randomActionProbabilityTargetValue) _randomActionProbabilityCurrentValue = _randomActionProbabilityTargetValue;
  }

  // Calculating effective value for random action probability
  _randomActionProbabilityEffectiveValue = _experienceReplay.size() > _experienceReplayStartSize ? _randomActionProbabilityCurrentValue : 1.0;
}

void Agent::normalizeStateActionNeuralNetwork(NeuralNetwork *neuralNetwork, size_t miniBatchSize, size_t normalizationSteps)
{
  // Allocating memory for the mini batch set
  std::vector<std::vector<std::vector<float>>> miniBatches(normalizationSteps);

  for (size_t i = 0; i < normalizationSteps; i++)
    miniBatches[i].resize(miniBatchSize);

  for (size_t i = 0; i < normalizationSteps; i++)
    for (size_t j = 0; j < miniBatchSize; j++)
      miniBatches[i][j].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // Creating minibatch to normalize from
  auto miniBatchIndexes = generateMiniBatch(miniBatchSize);

  // Filling the minibatches
  for (size_t step = 0; step < normalizationSteps; step++)
    for (size_t i = 0; i < miniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = miniBatchIndexes[i];

      // Getting current state and action
      auto curState = _experienceReplay[expId].state;
      auto curAction = _experienceReplay[expId].action;

      for (size_t j = 0; j < _problem->_stateVectorSize; j++) miniBatches[step][i][j] = curState[j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) miniBatches[step][i][_problem->_stateVectorSize + j] = curAction[j];
    }

  neuralNetwork->normalize(miniBatches);
}

void Agent::normalizeStateNeuralNetwork(NeuralNetwork *neuralNetwork, size_t miniBatchSize, size_t normalizationSteps)
{
  // Allocating memory for the mini batch set
  std::vector<std::vector<std::vector<float>>> miniBatches(normalizationSteps);

  for (size_t i = 0; i < normalizationSteps; i++)
    miniBatches[i].resize(miniBatchSize);

  for (size_t i = 0; i < normalizationSteps; i++)
    for (size_t j = 0; j < miniBatchSize; j++)
      miniBatches[i][j].resize(_problem->_stateVectorSize);

  // Creating storage for state history indexes to choose from
  std::vector<size_t> experienceReplayIndexes(_experienceReplay.size());
  for (size_t i = 0; i < _experienceReplay.size() - 1; i++) experienceReplayIndexes[i] = i;

  // Filling the minibatches
  for (size_t step = 0; step < normalizationSteps; step++)
  {
    // Shuffling indexes to choose the mini batch from
    std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

    for (size_t i = 0; i < miniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = experienceReplayIndexes[i];

      // Getting current state
      auto curState = _experienceReplay[expId].state;

      miniBatches[step][i] = curState;
    }
  }

  neuralNetwork->normalize(miniBatches);
}

void Agent::finalize()
{
 // Updating policy with the best parameters produced during training
 updateAgentPolicy(_bestHyperparameters);
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplay.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Current Reward:              %f/%f\n", _currentReward, _trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f (%lu)\n", _bestTrainingReward, _bestTrainingEpisode);
  _k->_logger->logInfo("Normal", " + Current p(Random Action):       %f\n", _randomActionProbabilityEffectiveValue);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Reward:       %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f\n", _bestAverageTestingReward);

  printAgentInformation();

  _k->_logger->logInfo("Normal", "Profiling Information:\n");
  _k->_logger->logInfo("Normal", " + NN Input Write Time:         %fs\n", _inputWriteTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Output Read Time:         %fs\n", _outputReadTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Normalization Time:       %fs\n", _normalizationTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Forward Propagation Time: %fs\n", _forwardStreamTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Back Propagation Time:    %fs\n", _backwardStreamTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + Agent Start Time:            %fs\n", _agentStartTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + Agent Wait Time:             %fs\n", _agentWaitTime / 1.0e+9f);
}

} // namespace solver

} // namespace korali
