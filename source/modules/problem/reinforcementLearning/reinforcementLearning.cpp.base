#include "engine.hpp"
#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

__startNamespace__;

/**
 * @brief Pointer to the current agent, it is immediately copied as to avoid concurrency problems
 */
Sample *__currentSample;

/**
 * @brief Identifier of the current environment function Id.
 */
size_t __envFunctionId;

/**
 * @brief Pointer to the agent (Korali solver module)
 */
solver::Agent *_agent;

/**
 * @brief Pointer to the engine's conduit
 */
Conduit *_conduit;

/**
 * @brief Stores the environment thread (coroutine).
 */
cothread_t _envThread;

/**
 * @brief Stores the current launch Id for the current sample
 */
size_t _launchId;

void __className__::initialize()
{
  // Processing state/action variable configuration
  _stateVectorIndexes.clear();
  _actionVectorIndexes.clear();
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
    if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
  }

  _stateVectorSize = _stateVectorIndexes.size();
  _actionVectorSize = _actionVectorIndexes.size();

  if (_actionVectorSize == 0) KORALI_LOG_ERROR("No action variables have been defined.\n");
  if (_stateVectorSize == 0) KORALI_LOG_ERROR("No state variables have been defined.\n");
  if ((_policiesPerEnvironment != _agentsPerEnvironment) && (_policiesPerEnvironment != 1))
    KORALI_LOG_ERROR("Number of Policies: %lu is neither 1 nor %lu.\n", _policiesPerEnvironment, _agentsPerEnvironment);

  // Validating observations
  _numberObservedTrajectories = _observationsStates.size();
  if (_numberObservedTrajectories == 0) KORALI_LOG_ERROR("No states have been recorded.\n");
  if (_observationsActions.size() == 0) KORALI_LOG_ERROR("No actions have been recorded.\n");
  if (_observationsFeatures.size() == 0) KORALI_LOG_ERROR("No features have been recorded.\n");
  if (_observationsStates[0].size() == 0) KORALI_LOG_ERROR("Observed states empty.\n");
  if (_observationsActions[0].size() == 0) KORALI_LOG_ERROR("Observed actions empty.\n");
  if (_observationsFeatures[0].size() == 0) KORALI_LOG_ERROR("Observed features empty.\n");

  _featureVectorSize = _observationsFeatures[0][0][0].size();
  if (_featureVectorSize == 0) KORALI_LOG_ERROR("No features have been defined.\n");

  if (_observationsActions.size() != _numberObservedTrajectories)
    KORALI_LOG_ERROR("Number of trajectories mismatch between observed states and observed actions.\n");

  if (_observationsFeatures.size() != _numberObservedTrajectories)
    KORALI_LOG_ERROR("Number of trajectories mismacht between observed states and observed features.\n");

  _totalObservedStateActionPairs = 0;
  for (size_t t = 0; t < _numberObservedTrajectories; ++t)
  {
    size_t trajectoryLength = _observationsStates[t].size();
    _totalObservedStateActionPairs += trajectoryLength * _agentsPerEnvironment;

    if (_observationsActions[t].size() != trajectoryLength)
      KORALI_LOG_ERROR("Trajectory (%zu) length of observed actions (%zu) does not agree with trajectory length of observed states (%zu)\n", t, _observationsActions[t].size(), trajectoryLength);

    if (_observationsFeatures[t].size() != trajectoryLength)
      KORALI_LOG_ERROR("Trajectory length of observed features (trajectory %zu) does not agree with observed states\n", t);

    for (size_t i = 0; i < trajectoryLength; ++i)
    {
      if (_observationsStates[t][i].size() != _agentsPerEnvironment)
        KORALI_LOG_ERROR("TODO");

      if (_observationsActions[t][i].size() != _agentsPerEnvironment)
        KORALI_LOG_ERROR("TODO");

      if (_observationsFeatures[t][i].size() != _agentsPerEnvironment)
        KORALI_LOG_ERROR("TODO");

      for (size_t a = 0; a < _agentsPerEnvironment; ++a)
      {
        if (_observationsStates[t][i][a].size() != _stateVectorSize)
          KORALI_LOG_ERROR("Dimension of observed state (trajectory %zu index %zu) does not agree with problem configuration.\n", t, i);

        if (_observationsActions[t][i][a].size() != _actionVectorSize)
          KORALI_LOG_ERROR("Dimension of observed action (%zu) does not agree with problem configuration (trajectory %zu index %zu).\n", _observationsActions[t][i].size(), t, i);

        if (_observationsFeatures[t][i][a].size() != _featureVectorSize)
          KORALI_LOG_ERROR("Dimension (%zu) of observed features (trajectory %zu index %zu) does not agree with problem configuration.\n", _observationsFeatures[t][i].size(), t, i, _featureVectorSize);
      }
    }
  }

  // Setting initial launch id (0)
  _launchId = 0;
}

/**
 * @brief Thread wrapper to run an environment
 */
void __environmentWrapper()
{
  Sample *worker = __currentSample;

  // Setting and increasing agent's launch Id
  (*worker)["Launch Id"] = _launchId++;
  worker->run(__envFunctionId);

  // If this is the leader rank within the worker group, check termination state
  if (_conduit->isWorkerLeadRank() != false)
  {
    if ((*worker)["Termination"] == "Non Terminal") KORALI_LOG_ERROR("Environment function terminated, but worker termination status (success or truncated) was not set.\n");

    bool terminationRecognized = false;
    if ((*worker)["Termination"] == "Terminal") terminationRecognized = true;
    if ((*worker)["Termination"] == "Truncated") terminationRecognized = true;

    if (terminationRecognized == false) KORALI_LOG_ERROR("Environment function terminated, but worker termination status (%s) is neither 'Terminal' nor 'Truncated'.\n", (*worker)["Termination"].get<std::string>().c_str());
  }

  // Returning to worker context
  co_switch(worker->_workerThread);

  KORALI_LOG_ERROR("Resuming a finished worker\n");
}

void __className__::runTrainingEpisode(Sample &worker)
{
  // Profiling information - Computation and communication time taken by the agent
  _agentPolicyEvaluationTime = 0.0;
  _agentComputationTime = 0.0;
  _agentCommunicationTime = 0.0;

  // Initializing environment configuration
  initializeEnvironment(worker);

  // Counter for the total number of actions taken
  size_t actionCount = 0;

  // Setting mode to traing to add exploratory noise or random actions
  worker["Mode"] = "Training";

  // Reserving message storage for sending back the episode
  knlohmann::json episode;

  // Storage to keep track of cumulative reward
  std::vector<float> trainingRewards(_agentsPerEnvironment, 0.0);

  // Setting termination status of initial state (and the following ones) to non terminal.
  // The environment will change this at the last state, indicating whether the episodes was
  // "Success" or "Truncated".
  worker["Termination"] = "Non Terminal";

  // Getting first state
  runEnvironment(worker);

  // If this is not the leader rank within the worker group, return immediately
  if (_k->_engine->_conduit->isWorkerLeadRank() == false)
  {
    finalizeEnvironment();
    return;
  }

  const auto policy = _agent->getPolicy();
  episode["Policy Hyperparameters"] = policy["Policy Hyperparameters"][0];

  // Saving experiences
  while (worker["Termination"] == "Non Terminal")
  {
    // Generating new action from the agent's policy
    getAction(worker);

    // Store the current state in the experience
    episode["Experiences"][actionCount]["State"] = worker["State"];

    // Store the current state in the experience
    episode["Experiences"][actionCount]["Features"] = worker["Features"];

    // Storing the current action
    episode["Experiences"][actionCount]["Action"] = worker["Action"];

    // Storing the experience's policy
    episode["Experiences"][actionCount]["Policy"] = worker["Policy"];

    // If single agent, put action into a single vector
    if (_agentsPerEnvironment == 1) worker["Action"] = worker["Action"][0].get<std::vector<float>>();

    // Jumping back into the worker's environment
    runEnvironment(worker);

    // In case of this being a single agent, revert action format
    if (_agentsPerEnvironment == 1)
    {
      auto action = KORALI_GET(std::vector<float>, worker, "Action");
      worker._js.getJson().erase("Action");
      worker["Action"][0] = action;
    }

    // Storing experience's reward
    episode["Experiences"][actionCount]["Reward"] = worker["Reward"];

    // Storing termination status
    episode["Experiences"][actionCount]["Termination"] = worker["Termination"];

    // If the episode was truncated, then save the terminal state
    if (worker["Termination"] == "Truncated")
    {
      episode["Experiences"][actionCount]["Truncated State"] = worker["State"];
    }

    // Adding to cumulative training rewards
    for (size_t i = 0; i < _agentsPerEnvironment; i++)
      trainingRewards[i] += worker["Reward"][i].get<float>();

    // Increasing counter for generated actions
    actionCount++;

    // Checking if we requested the given number of actions in between policy updates and it is not a terminal state
    if ((_actionsBetweenPolicyUpdates > 0) &&
        (worker["Termination"] == "Non Terminal") &&
        (actionCount % _actionsBetweenPolicyUpdates == 0))
    {
      requestNewPolicy(worker);
    }
  }

  // Setting cumulative reward
  worker["Training Rewards"] = trainingRewards;

  // Finalizing Environment
  finalizeEnvironment();

  // Setting tested policy flag to false, unless we do testing
  worker["Tested Policy"] = false;

  // Get current "true" episode count
  size_t episodeCount = worker["Sample Id"];

  // If the training reward of all the agents exceeds the threshold or meets the periodic conditions, then also run testing on it
  bool runTest = (_testingFrequency > 0) && (episodeCount % _testingFrequency == 0);

  if (runTest)
  {
    float averageTestingReward = 0.0;
    float bestTestingReward = -Inf;
    float worstTestingReward = +Inf;

    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      runTestingEpisode(worker);

      // Getting current testing reward
      auto currentTestingReward = KORALI_GET(float, worker, "Testing Reward");

      // Adding current testing reward to the average and keeping statistics
      averageTestingReward += currentTestingReward;
      if (currentTestingReward > bestTestingReward) bestTestingReward = currentTestingReward;
      if (currentTestingReward < worstTestingReward) worstTestingReward = currentTestingReward;
    }

    // Normalizing average
    averageTestingReward /= (float)_policyTestingEpisodes;

    // Storing testing information
    worker["Average Testing Reward"] = averageTestingReward;
    worker["Best Testing Reward"] = bestTestingReward;
    worker["Worst Testing Reward"] = worstTestingReward;

    // Indicate that the agent has been tested
    worker["Tested Policy"] = true;
  }

  // Sending last experience last (after testing)
  // This is important to prevent the engine for block-waiting for the return of the sample
  // while the testing runs are being performed.
  knlohmann::json message;
  message["Action"] = "Send Episodes";
  message["Sample Id"] = worker["Sample Id"];
  message["Episodes"] = episode;
  KORALI_SEND_MSG_TO_ENGINE(message);

  // Adding profiling information to worker
  worker["Computation Time"] = _agentComputationTime;
  worker["Communication Time"] = _agentCommunicationTime;
  worker["Policy Evaluation Time"] = _agentPolicyEvaluationTime;
}

void __className__::runTestingEpisode(Sample &worker)
{
  std::vector<float> testingRewards(_agentsPerEnvironment, 0.0);

  // Initializing Environment
  initializeEnvironment(worker);

  // Setting mode to testing to prevent the addition of noise or random actions
  worker["Mode"] = "Testing";

  // Setting initial non terminal state
  worker["Termination"] = "Non Terminal";

  // Getting first state
  runEnvironment(worker);

  // If this is not the leader rank within the worker group, return immediately
  if (_k->_engine->_conduit->isWorkerLeadRank() == false)
  {
    finalizeEnvironment();
    return;
  }

  // Running environment using the last policy only
  while (worker["Termination"] == "Non Terminal")
  {
    getAction(worker);

    // If single agent, put action into a single vector
    // In case of this being a single agent, support returning action as only vector
    if (_agentsPerEnvironment == 1) worker["Action"] = worker["Action"][0].get<std::vector<float>>();

    runEnvironment(worker);

    // In case of this being a single agent, revert action format
    if (_agentsPerEnvironment == 1)
    {
      auto action = KORALI_GET(std::vector<float>, worker, "Action");
      worker._js.getJson().erase("Action");
      worker["Action"][0] = action;
    }

    for (size_t i = 0; i < _agentsPerEnvironment; i++)
      testingRewards[i] += worker["Reward"][i].get<float>();
  }

  // Calculating average reward between testing episodes
  float rewardSum = 0.0f;
  for (size_t i = 0; i < _agentsPerEnvironment; i++)
    rewardSum += testingRewards[i];

  // Storing the average cumulative reward among agents of the testing episode
  worker["Testing Reward"] = rewardSum / _agentsPerEnvironment;

  // Finalizing Environment
  finalizeEnvironment();
}

void __className__::initializeEnvironment(Sample &worker)
{
  // Getting RL-compatible solver
  _agent = dynamic_cast<solver::Agent *>(_k->_solver);

  // Getting worker's conduit
  _conduit = _agent->_k->_engine->_conduit;

  // First, we update the initial policy's hyperparameters
  _agent->setPolicy(worker["Policy Hyperparameters"]);

  // Then, we reset the state sequence for time-dependent learners
  _agent->resetTimeSequence();

  // Define state rescaling variables
  _stateRescalingMeans = worker["State Rescaling"]["Means"].get<std::vector<std::vector<float>>>();
  _stateRescalingSdevs = worker["State Rescaling"]["Standard Deviations"].get<std::vector<std::vector<float>>>();

  // Appending any user-defined settings
  worker["Custom Settings"] = _customSettings;

  // Creating agent coroutine
  __currentSample = &worker;
  __envFunctionId = _environmentFunction;
  worker._workerThread = co_active();

  // Creating coroutine
  _envThread = co_create(1 << 28, __environmentWrapper);

  // Initializing rewards
  if (_agentsPerEnvironment == 1) worker["Reward"] = 0.0f;
  if (_agentsPerEnvironment > 1) worker["Reward"] = std::vector<float>(_agentsPerEnvironment, 0.0f);
}

void __className__::finalizeEnvironment()
{
  // Freeing training co-routine memory
  co_delete(_envThread);
}

void __className__::requestNewPolicy(Sample &worker)
{
  auto t0 = std::chrono::steady_clock::now(); // Profiling

  // Reserving message storage for requesting new policy
  knlohmann::json message;

  // Sending request to engine
  message["Sample Id"] = worker["Sample Id"];
  message["Action"] = "Request New Policy";
  KORALI_SEND_MSG_TO_ENGINE(message);

  // If requested new policy, wait for incoming message containing new hyperparameters
  worker["Policy Hyperparameters"] = KORALI_RECV_MSG_FROM_ENGINE();
  _agent->setPolicy(worker["Policy Hyperparameters"]);

  auto t1 = std::chrono::steady_clock::now();                                                       // Profiling
  _agentCommunicationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(t1 - t0).count(); // Profiling
}

void __className__::getAction(Sample &worker)
{
  // Generating new action from policy
  auto t0 = std::chrono::steady_clock::now(); // Profiling

  _agent->getAction(worker);

  auto t1 = std::chrono::steady_clock::now();                                                          // Profiling
  _agentPolicyEvaluationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(t1 - t0).count(); // Profiling
}

void __className__::runEnvironment(Sample &worker)
{
  // Switching back to the environment's thread
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  co_switch(_envThread);
  auto endTime = std::chrono::steady_clock::now();                                                            // Profiling
  _agentComputationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

  // In case of this being a single worker, preprocess state and reward if necessary
  if (_conduit->isWorkerLeadRank() == false) return;

  // Define feature rescaling variables
  const auto featureRescalingMeans = worker["Feature Rescaling"]["Means"].get<std::vector<std::vector<float>>>();
  const auto featureRescalingSdevs = worker["Feature Rescaling"]["Standard Deviations"].get<std::vector<std::vector<float>>>();

  // Define state rescaling variables
  const auto stateRescalingMeans = worker["State Rescaling"]["Means"].get<std::vector<std::vector<float>>>();
  const auto stateRescalingSdevs = worker["State Rescaling"]["Standard Deviations"].get<std::vector<std::vector<float>>>();

  // In case of this being a single agent, support returning state as only vector
  if (_agentsPerEnvironment == 1)
  {
    const auto features = KORALI_GET(std::vector<float>, worker, "Features");
    worker._js.getJson().erase("Features");
    worker["Features"][0] = features;
    // Support returning state as vector
    const auto state = KORALI_GET(std::vector<float>, worker, "State");
    worker._js.getJson().erase("State");
    worker["State"][0] = state;

    // Support returning reward as scalar
    const auto reward = KORALI_GET(float, worker, "Reward");
    worker._js.getJson().erase("Reward");
    worker["Reward"][0] = reward;
  }

  // Checking correct format of state
  if (worker["State"].is_array() == false) KORALI_LOG_ERROR("Agent state variable returned by the environment is not a vector.\n");
  if (worker["State"].size() != _agentsPerEnvironment) KORALI_LOG_ERROR("Agents state vector returned with the wrong size: %lu, expected: %lu.\n", worker["State"].size(), _agentsPerEnvironment);

  if (worker["Features"].is_array() == false) KORALI_LOG_ERROR("Agent feature variable returned by the environment is not a vector.\n");
  if (worker["Features"].size() != _agentsPerEnvironment) KORALI_LOG_ERROR("Agents feature vector returned with the wrong size: %lu, expected: %lu.\n", worker["Features"].size(), _agentsPerEnvironment);

  // Sanity checks for state and features
  for (size_t i = 0; i < _agentsPerEnvironment; ++i)
  {
    if (worker["State"][i].is_array() == false) KORALI_LOG_ERROR("Agent state variable returned by the environment is not a vector.\n");
    if (worker["State"][i].size() != _stateVectorSize) KORALI_LOG_ERROR("Agents state vector %lu returned with the wrong size: %lu, expected: %lu.\n", i, worker["State"][i].size(), _stateVectorSize);

    // Scale the state
    for (size_t d = 0; d < _stateVectorSize; ++d)
    {
      worker["State"][i][d] = (worker["State"][i][d].get<float>() - stateRescalingMeans[i][d]) / stateRescalingSdevs[i][d];
      if (std::isfinite(worker["State"][i][d].get<float>()) == false) KORALI_LOG_ERROR("Agent %lu state variable %lu returned an invalid value: %f\n", i, d, worker["State"][i][d].get<float>());
    }

    if (worker["Features"][i].is_array() == false) KORALI_LOG_ERROR("Agent feature variable returned by the environment is not a vector.\n");
    if (worker["Features"][i].size() != _featureVectorSize) KORALI_LOG_ERROR("Agents feature vector %lu returned with the wrong size: %lu, expected: %lu.\n", i, worker["Features"][i].size(), _featureVectorSize);

    // Scale features
    for (size_t d = 0; d < _featureVectorSize; ++d)
    {
      worker["Features"][i][d] = (worker["Features"][i][d].get<float>() - featureRescalingMeans[i][d]) / featureRescalingSdevs[i][d];
      if (std::isfinite(worker["Features"][i][d].get<float>()) == false) KORALI_LOG_ERROR("Agent %lu feature variable %lu returned an invalid value: %f\n", i, d, worker["Features"][i][d].get<float>());
    }
  }

  // Checking correct format of reward
  if (worker["Reward"].is_array() == false) KORALI_LOG_ERROR("Agent reward variable returned by the environment is not a vector.\n");
  if (worker["Reward"].size() != _agentsPerEnvironment) KORALI_LOG_ERROR("Agents reward vector returned with the wrong size: %lu, expected: %lu.\n", worker["Reward"].size(), _agentsPerEnvironment);

  for (size_t i = 0; i < _agentsPerEnvironment; i++)
    if (std::isfinite(worker["Reward"][i].get<float>()) == false) KORALI_LOG_ERROR("Agent %lu reward returned an invalid value: %f\n", i, worker["Reward"][i].get<float>());

  // If available actions not given, set all 1s
  std::vector<size_t> availableActions(_actionCount, 1);
  if (not isDefined(worker._js.getJson(), "Available Actions"))
  {
    for (size_t i = 0; i < _agentsPerEnvironment; i++)
    {
      worker["Available Actions"][i] = availableActions;
    }
  }

  // Check format of available action
  if (worker["Available Actions"].size() != _agentsPerEnvironment) KORALI_LOG_ERROR("Available Actions vector returned with the wrong size: %lu, expected: %lu.\n", worker["Available Actions"].size(), _agentsPerEnvironment);

  for (size_t i = 0; i < _agentsPerEnvironment; i++)
  {
    if (worker["Available Actions"][i].size() != _actionCount) KORALI_LOG_ERROR("Available Actions vector %lu returned with the wrong size: %lu, expected: %lu.\n", i, worker["Available Actions"][i].size(), _actionCount);
  }
}

__moduleAutoCode__;

__endNamespace__;
