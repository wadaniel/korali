{

 "Configuration Settings":
 [
   {
   "Name": [ "Mode" ],
   "Type": "std::string",
   "Options": [
               { "Value": "Training", "Description": "Learns a policy for the reinforcement learning problem." },
               { "Value": "Testing", "Description": "Tests the policy with a learned policy." }
              ],
   "Description": "Specifies the operation mode for the agent."
  },
  {
    "Name": [ "Testing", "Sample Ids" ],
    "Type": "std::vector<size_t>",
    "Description": "A vector with the identifiers for the samples to test the hyperparameters with."
  },
  {
    "Name": [ "Testing", "Policy" ],
    "Type": "knlohmann::json",
    "Description": "The hyperparameters of the policy to test."
  },
  {
    "Name": [ "Training", "Average Depth" ],
    "Type": "size_t",
    "Description": "Specifies the depth of the running training average to report."
  },
  {
    "Name": [ "Agent Count" ],
    "Type": "size_t", 
    "Description": "Indicates the number of concurrent agents collecting experiences."
  },
  {
    "Name": [ "Episodes Per Generation" ],
    "Type": "size_t", 
    "Description": "Indicates the how many finished episodes to receive in a generation (checkpoints are generated between generations)."
  },
  {
    "Name": [ "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
    "Name": [ "Time Sequence Length" ],
    "Type": "size_t", 
    "Description": "Indicates the number of contiguous experiences to pass to the NN for learning. This is only useful when using recurrent NNs on problems with time-dependent phenomena."
  },
  {
   "Name": [ "Optimizer" ],
   "Type": "std::string",
   "Options": [
            { "Value": "Adam", "Description": "Uses the Adam optimizer." },
            { "Value": "AdaBelief", "Description": "Uses the AdaBelief optimizer." }
           ],
   "Description": "Determines which optimizer algorithm to use for learning the NN hyperparameters."
  },
  {
    "Name": [ "Learning Rate" ],
    "Type": "float",
    "Description": "The base learning rate to use for the NN hyperparameter optimization."
  },
  {
   "Name": [ "L2 Regularization", "Enabled" ],
   "Type": "bool",
   "Description": "Regulates if l2 regularization will be applied to the neural networks."
  },
  {
   "Name": [ "L2 Regularization", "Importance" ],
   "Type": "bool",
   "Description": "Importance weight of l2 regularization."
  },
  {
    "Name": [ "Neural Network", "Hidden Layers" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the hidden neural network layers."
  },
  {
   "Name": [ "Neural Network", "Engine" ],
   "Type": "std::string", 
   "Description": "Specifies which Neural Network backend engine to use."
  },
  {
   "Name": [ "Discount Factor" ],
   "Type": "float",
   "Description": "Represents the weight given to the expectation of the cumulative reward from future experiences."
  },
  {
    "Name": [ "Experience Replay", "Serialize" ],
    "Type": "bool",
    "Description": "Indicates whether to serialize and store the experience replay after each generation file save. Disabling will reduce I/O overheads but will disable the checkpoint/resume function."
  },
  {
    "Name": [ "Experience Replay", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Experience Replay", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Experience Replay", "Importance Weight", "Annealing Rate" ],
    "Type": "float",
    "Description": "Annealing factor for prioritized experience replay (1.0 full compensation, 0.0 uniform treatment)."
  },
  {
    "Name": [ "Experience Replay", "Priority Annealing Rate" ],
    "Type": "float",
    "Description": "Annealing rate for experience probability calculation (1.0 full rank based probabilities, 0.0 uniform probabilities)."
  },
   {
    "Name": [ "Experience Replay", "Off Policy", "Cutoff Scale" ],
    "Type": "float",
    "Description": "Indicates the scaling of the importance weight threshold under/above which an experience is considered off-policy. "
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Target" ],
    "Type": "float",
    "Description": "Indicates the target off-policiness. The agent will try to stay under this target value"
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Annealing Rate" ],
    "Type": "float",
    "Description": "The parameters A from eq. (13) for REFER  (see: https://arxiv.org/abs/1807.05827) for controling experience off-policiness in the ER memory."
  },
  {
   "Name": [ "Experience Replay", "Off Policy", "REFER Beta" ],
   "Type": "float",
   "Description": "Initial value for the penalisation coefficient for off-policiness. "
  },
  {
    "Name": [ "Experiences Between Policy Updates" ],
    "Type": "float",
    "Description": "The number of experiences to receive before training/updating (real number, may be less than < 1.0, for more than one update per experience)."
  },
  {
   "Name": [ "Mini Batch Strategy" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Uniform", "Description": "Selects experiences from the replay memory with a randomly." },
      { "Value": "Prioritized", "Description": "Prioritizes experiences according to their rank. See  Prioritized Experience Replay by Schaul et al. (2015)." },
      { "Value": "Remember-and-Forget", "Description": "Selects experiences randomly, so long as their importance weight is above a given tolerance." }
     ],
   "Description": "Determines how to select experiences from the replay memory for mini batch creation."
  },  
  {
   "Name": [ "Cache Persistence" ],
   "Type": "size_t",
   "Description": "Indicates for how many policy updates will pre-calculated values be persist in the experience cache."
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxExperiences > 0) && (_experienceReplay.size() >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Testing", "Target Average Reward" ],
    "Type": "float",
    "Criteria": "(_mode == \"Training\") && (_testingTargetAverageReward > -korali::Inf) && (_testingBestAverageReward >= _testingTargetAverageReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  },
  {
    "Name": [ "Testing", "Average Reward Increment" ],
    "Type": "float",
    "Criteria": "(_mode == \"Training\") && (_testingAverageRewardIncrement > 0.0) && (_testingPreviousAverageReward > -korali::Inf) && (_testingAverageReward + _testingStdevReward * _testingAverageRewardIncrement < _testingPreviousAverageReward)",
    "Description": "The solver will stop when the averge testing reward is below the previous testing average by more than a threshold given by this factor multiplied with the testing standard deviation."
  },
  {
    "Name": [ "Max Policy Updates" ],
    "Type": "size_t", 
    "Criteria": "(_mode == \"Training\") && (_maxPolicyUpdates > 0) && (_policyUpdateCount >= _maxPolicyUpdates)",
    "Description": "The solver will stop when the given number of optimizations have been made to the learner."
  }
 ],

 "Variables Configuration":
 [

 ],

 "Internal Settings":
 [
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
   "Name": [ "Last Training Reward" ],
   "Type": "float",
   "Description": "The cumulative training reward for the last episode received."
  },
  {
    "Name": [ "Training", "Reward History" ],
    "Type": "std::vector<float>",
    "Description": "Keeps a history of all training episode rewards."
  },
  {
    "Name": [ "Training", "Experience History" ],
    "Type": "std::vector<size_t>",
    "Description": "Keeps a history of all training episode experience counts."
  },
  {
    "Name": [ "Training", "Average Reward" ],
    "Type": "float",
    "Description": "Contains a running average of the training episode rewards."
  },
  {
    "Name": [ "Training", "Last Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative reward of the last training episode."
  },
  {
    "Name": [ "Training", "Best Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative average episode reward found so far from training episodes."
  },
  {
    "Name": [ "Training", "Best Episode Id" ],
    "Type": "size_t",
    "Description": "Remembers the episode that obtained the maximum reward found so far during training."
  },
  {
    "Name": [ "Testing", "Reward" ],
    "Type": "std::vector<float>",
    "Description": "The rewards obtained when evaluating the testing samples."
  },
  {
    "Name": [ "Testing", "Best Reward" ],
    "Type": "float",
    "Description": "Remembers the best reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Worst Reward" ],
    "Type": "float",
    "Description": "Remembers the worst reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Best Episode Id" ],
    "Type": "size_t",
    "Description": "Remembers the episode Id that obtained the maximum reward found so far during testing."
  },
  {
    "Name": [ "Testing", "Candidate Count" ],
    "Type": "size_t",
    "Description": "Remembers the number of candidate policies tested so far."
  },
  {
    "Name": [ "Testing", "Average Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Stdev Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Previous Average Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from previous testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Best Average Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative average episode reward found so far from testing episodes."
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Ratio" ],
    "Type": "float",
    "Description": "Current off policy ratio in the experience replay."
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Current Cutoff" ],
    "Type": "float",
    "Description": "Indicates the current importance weight threshold under which an experience is considered off-policy. "
  },
  {
    "Name": [ "Current Learning Rate" ],
    "Type": "float",
    "Description": "The current learning rate to use for the NN hyperparameter optimization."
  },
  {
   "Name": [ "Policy Update Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of policy updates performed."
  },
  {
   "Name": [ "Current Sample ID" ],
   "Type": "size_t",
   "Description": "Keeps track of the current Sample ID, to make sure no two equal sample IDs are produced and that this value can be used as random seed."
  },
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator for epsilon-greedy strategy."
  },
  {
    "Name": [ "Experience Count" ],
    "Type": "size_t",
    "Description": "Takes count of the number of experiences received so far."
  },
  {
    "Name": [ "Experience Replay", "Max Priority" ],
    "Type": "size_t",
    "Description": "Indicates the maximum priority of any experience in the experience replay."
  },
  {
    "Name": [ "Feature Weights" ],
    "Type": "std::vector<float>",
    "Description": "The weights of the features required to calculate the reward."
  }
 ],

 "Module Defaults":
 {
   "Episodes Per Generation": 1,
   "Agent Count": 1,
   "Optimizer": "AdaBelief",
   "Discount Factor": 0.99,
   "Mini Batch Strategy": "Uniform",
   "Cache Persistence": 0,
   "Time Sequence Length": 1,
   
   "L2 Regularization": 
   {
     "Enabled": false,
     "Importance": 1e-4
   },
   
   "Training":
   {
    "Average Depth": 100
   },
   
   "Testing":
   {
    "Sample Ids": [ ],
    "Policy": { }
   }, 
   
   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Policy Updates": 0,
    "Testing":
    {
     "Target Average Reward": -Infinity,
     "Average Reward Increment": 0.0
    } 
   },
   
  "Experience Replay":
   {
    "Serialize": true,
    "Maximum Size": 10000,
    "Start Size": 1000,
    "Priority Annealing Rate" : 1.0,
    "Database": [ ],
    
    "Off Policy":
    {
     "Cutoff Scale": 4.0,
     "Target": 0.1,
     "REFER Beta": 0.3,
     "Annealing Rate": 5e-7
    },
    
    "Importance Weight":
    { 
     "Annealing Rate" : 1.0
    }
   },
   
   "Uniform Generator":
   {
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   }
 },
 
 "Variable Defaults":
 {
 
 }
 
}
