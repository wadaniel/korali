#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"
#include <gsl/gsl_multifit.h>
#include <gsl/gsl_sf_psi.h>

__startNamespace__;

void __className__::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  // Obtaining action shift and scales for bounded distributions
  _actionShifts.resize(_problem->_actionVectorSize);
  _actionScales.resize(_problem->_actionVectorSize);
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    // For bounded distributions, infinite bounds should result in an error message
    if (_policyDistribution == "Squashed Normal" || _policyDistribution == "Beta" || _policyDistribution == "Clipped Normal" || _policyDistribution == "Truncated Normal")
    {
      if (isfinite(_actionLowerBounds[i]) == false)
        KORALI_LOG_ERROR("Provided lower bound (%f) for action variable %lu is non-finite, but the distribution (%s) is bounded.\n", _actionLowerBounds[i], i, _policyDistribution.c_str());

      if (isfinite(_actionUpperBounds[i]) == false)
        KORALI_LOG_ERROR("Provided upper bound (%f) for action variable %lu is non-finite, but the distribution (%s) is bounded.\n", _actionUpperBounds[i], i, _policyDistribution.c_str());

      _actionShifts[i] = (_actionUpperBounds[i] + _actionLowerBounds[i]) * 0.5f;
      _actionScales[i] = (_actionUpperBounds[i] - _actionLowerBounds[i]) * 0.5f;
    }
  }

  // Obtaining policy parameter transformations (depends on which policy distribution chosen)
  if (_policyDistribution == "Normal" || _policyDistribution == "Squashed Normal" || _policyDistribution == "Clipped Normal" || _policyDistribution == "Truncated Normal")
  {
    _policyParameterCount = 2 * _problem->_actionVectorSize; // Mus and Sigmas

    // Allocating space for the required transformations
    _policyParameterTransformationMasks.resize(_policyParameterCount);
    _policyParameterScaling.resize(_policyParameterCount);
    _policyParameterShifting.resize(_policyParameterCount);

    // Establishing transformations for the Normal policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const auto varIdx = _problem->_actionVectorIndexes[i];
      const float sigma = _k->_variables[varIdx]->_initialExplorationNoise;

      // Checking correct noise configuration
      if (sigma <= 0.0f) KORALI_LOG_ERROR("Provided initial noise (%f) for action variable %lu is not defined or negative.\n", sigma, varIdx);

      // Identity mask for Means
      _policyParameterScaling[i] = 1.0; //_actionScales[i];
      _policyParameterShifting[i] = _actionShifts[i];
      _policyParameterTransformationMasks[i] = "Identity";

      // Softplus mask for Sigmas
      _policyParameterScaling[_problem->_actionVectorSize + i] = 2.0f * sigma;
      _policyParameterShifting[_problem->_actionVectorSize + i] = 0.0f;
      _policyParameterTransformationMasks[_problem->_actionVectorSize + i] = "Softplus"; // 0.5 * (x + sqrt(1 + x*x))
    }
  }

  if (_policyDistribution == "Beta")
  {
    _policyParameterCount = 2 * _problem->_actionVectorSize; // Mu and Variance

    // Allocating space for the required transformations
    _policyParameterTransformationMasks.resize(_policyParameterCount);
    _policyParameterScaling.resize(_policyParameterCount);
    _policyParameterShifting.resize(_policyParameterCount);

    // Establishing transformations for the Normal policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const size_t varIdx = _problem->_actionVectorIndexes[i];
      const float sigma = _k->_variables[varIdx]->_initialExplorationNoise;

      // Checking correct noise configuration
      if (sigma <= 0.0f) KORALI_LOG_ERROR("Provided initial noise (%f) for action variable %lu is not defined or negative.\n", sigma, varIdx);

      // Identity mask for Means
      _policyParameterScaling[i] = 1.0f;
      _policyParameterShifting[i] = _actionShifts[i];
      _policyParameterTransformationMasks[i] = "Identity";

      // Sigmoid Mask for Variance
      _policyParameterTransformationMasks[_problem->_actionVectorSize + i] = "Sigmoid";
      _policyParameterScaling[_problem->_actionVectorSize + i] = 2.0f * sigma;
      _policyParameterShifting[_problem->_actionVectorSize + i] = 0.0f;
    }
  }

  // Allocate memory for linear controller
  _observationsApproximatorWeights.resize(_problem->_actionVectorSize, std::vector<float>(_problem->_stateVectorSize + 1));
  _observationsApproximatorSigmas.resize(_problem->_actionVectorSize);

  // Building linear controller for observed state action pairs
  gsl_matrix *X = gsl_matrix_alloc(_problem->_totalObservedStateActionPairs, _problem->_stateVectorSize + 1);
  gsl_matrix *Y = gsl_matrix_alloc(_problem->_totalObservedStateActionPairs, _problem->_actionVectorSize);
  size_t idx = 0;
  for (size_t i = 0; i < _problem->_numberObservedTrajectories; ++i)
  {
    size_t trajectoryLength = _problem->_observationsStates[i].size();
    for (size_t t = 0; t < trajectoryLength; ++t)
    {
      gsl_matrix_set(X, idx, 0, 1.0); // intercept
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
      {
        gsl_matrix_set(X, idx, j + 1, (double)_problem->_observationsStates[i][t][j]);
      }
      for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
      {
        gsl_matrix_set(Y, idx, k, (double)_problem->_observationsActions[i][t][k]);
      }
      idx++;
    }
  }

  _k->_logger->logInfo("Normal", "Linear Approximator Expert Policy\n");

  // Do regression over actions
  for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
  {
    double chisq;
    gsl_vector *c = gsl_vector_alloc(_problem->_stateVectorSize + 1);
    gsl_matrix *cov = gsl_matrix_alloc(_problem->_stateVectorSize + 1, _problem->_stateVectorSize + 1);
    gsl_multifit_linear_workspace *work = gsl_multifit_linear_alloc(_problem->_totalObservedStateActionPairs, _problem->_stateVectorSize + 1);

    // predict action Y_k
    gsl_vector_view y = gsl_matrix_column(Y, k);
    gsl_multifit_linear(X, &y.vector, c, cov, &chisq, work);

    for (size_t j = 0; j < _problem->_stateVectorSize + 1; ++j)
    {
      _observationsApproximatorWeights[k][j] = gsl_vector_get(c, j);
      _k->_logger->logInfo("Normal", "    + Weights  [%zu, %zu] %f \n", k, j, _observationsApproximatorWeights[k][j]);
    }

    gsl_multifit_linear_free(work);
    gsl_vector_free(c);
    gsl_matrix_free(cov);
  }

  // Calculate squared error over all predictions
  std::vector<float> squaredErrors(_problem->_actionVectorSize);
  for (size_t t = 0; t < _problem->_numberObservedTrajectories; ++t)
    for (size_t i = 0; i < _problem->_observationsStates[t].size(); ++i)
    {
      for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
      {
        float approx = _observationsApproximatorWeights[k][0]; // intercept
        for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        {
          approx += _problem->_observationsStates[t][i][j] * _observationsApproximatorWeights[k][j + 1];
        }
        squaredErrors[k] += std::pow(_problem->_observationsActions[t][i][k] - approx, 2.);
      }
    }

  // Set MLE sigma estimates
  for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
  {
    _observationsApproximatorSigmas[k] = std::sqrt(squaredErrors[k] / (float)_problem->_totalObservedStateActionPairs);
    _k->_logger->logInfo("Normal", "    + Sigma    [%zu] %f \n", k, _observationsApproximatorSigmas[k]);
  }

  gsl_matrix_free(X);
  gsl_matrix_free(Y);
}

void __className__::getAction(korali::Sample &sample)
{
  // Get action for all the agents in the environment
  for (size_t i = 0; i < sample["State"].size(); i++)
  {
    // Getting current state
    auto state = sample["State"][i];

    // Adding state to the state time sequence
    _stateTimeSequence.add(state);

    // Storage for the action to select
    std::vector<float> action(_problem->_actionVectorSize);

    // Forward state sequence to get the Gaussian means and sigmas from policy
    auto policy = runPolicy({_stateTimeSequence.getVector()})[0];

    /*****************************************************************************
     * During Training we select action according to policy's probability
     * distribution
     ****************************************************************************/

    if (sample["Mode"] == "Training") action = generateTrainingAction(policy);

    /*****************************************************************************
     * During testing, we select the means (point of highest density) for all
     * elements of the action vector
     ****************************************************************************/

    if (sample["Mode"] == "Testing") action = generateTestingAction(policy);

    /*****************************************************************************
     * Storing the action and its policy
     ****************************************************************************/

    sample["Policy"][i]["Distribution Parameters"] = policy.distributionParameters;
    sample["Policy"][i]["State Value"] = policy.stateValue;
    sample["Policy"][i]["Unbounded Action"] = policy.unboundedAction;
    sample["Action"][i] = action;
  }
}

std::vector<float> __className__::generateTrainingAction(policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  // Creating the action based on the selected policy
  if (_policyDistribution == "Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mean = curPolicy.distributionParameters[i];
      const float sigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      action[i] = mean + sigma * _normalGenerator->getRandomNumber();
    }
  }

  if (_policyDistribution == "Squashed Normal")
  {
    std::vector<float> unboundedAction(_problem->_actionVectorSize);
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mu = curPolicy.distributionParameters[i];
      const float sigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float scale = _actionScales[i];
      const float shift = _actionShifts[i];

      unboundedAction[i] = mu + sigma * _normalGenerator->getRandomNumber();
      action[i] = (std::tanh(unboundedAction[i]) * scale) + shift;

      // Safety check
      if (action[i] >= _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
      if (action[i] <= _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
    }
    curPolicy.unboundedAction = unboundedAction;
  }

  if (_policyDistribution == "Clipped Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mu = curPolicy.distributionParameters[i];
      const float sigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      action[i] = mu + sigma * _normalGenerator->getRandomNumber();

      if (action[i] >= _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
      if (action[i] <= _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
    }
  }

  if (_policyDistribution == "Truncated Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mu = curPolicy.distributionParameters[i];
      const float sigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float alpha = (_actionLowerBounds[i] - mu) / sigma;
      const float beta = (_actionUpperBounds[i] - mu) / sigma;

      // Sampling via naive inverse sampling (not the safest approach due to numerical precision)
      const float u = _uniformGenerator->getRandomNumber();
      const float z = u * normalCDF(beta, 0.f, 1.f) + (1. - u) * normalCDF(alpha, 0.f, 1.f);
      action[i] = mu + M_SQRT2 * ierf(2. * z - 1.) * sigma;

      // Safety check
      if (action[i] >= _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
      if (action[i] <= _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      action[i] = ranBetaAlt(_normalGenerator->_range, curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
    }
  }

  return action;
}

std::vector<float> __className__::generateTestingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  if (_policyDistribution == "Normal")
  {
    // Take only the means without noise
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = curPolicy.distributionParameters[i];
  }

  if (_policyDistribution == "Squashed Normal")
  {
    // Take only the transformed means without noise
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mu = curPolicy.distributionParameters[i];
      const float scale = _actionScales[i];
      const float shift = _actionShifts[i];
      action[i] = (std::tanh(mu) * scale) + shift;
    }
  }

  if (_policyDistribution == "Clipped Normal")
  {
    // Take only the modes of the Clipped Normal without noise
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      action[i] = curPolicy.distributionParameters[i];
      // Clip mode to bounds
      if (action[i] >= _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
      if (action[i] <= _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
    }
  }

  if (_policyDistribution == "Truncated Normal")
  {
    // Take only the modes of the Truncated Normal noise
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      action[i] = curPolicy.distributionParameters[i];
      // Clip mode to bounds
      if (action[i] >= _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
      if (action[i] <= _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
    }
  }

  if (_policyDistribution == "Beta")
  {
    // Take only the modes without noise
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      action[i] = _actionLowerBounds[i] + 2.0f * _actionScales[i] * curPolicy.distributionParameters[i];
    }
  }

  return action;
}

float __className__::calculateImportanceWeight(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  float logpCurPolicy = 0.0f;
  float logpOldPolicy = 0.0f;

  if (_policyDistribution == "Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      logpCurPolicy += normalLogDensity(action[i], curMean, curSigma);
      logpOldPolicy += normalLogDensity(action[i], oldMean, oldSigma);
    }
  }

  if (_policyDistribution == "Squashed Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      // Importance weight of squashed normal is the importance weight of normal evaluated at unbounded action
      logpCurPolicy += normalLogDensity(oldPolicy.unboundedAction[i], curMu, curSigma);
      logpOldPolicy += normalLogDensity(oldPolicy.unboundedAction[i], oldMu, oldSigma);
    }
  }

  if (_policyDistribution == "Clipped Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      if (action[i] <= _actionLowerBounds[i])
      {
        logpCurPolicy += normalLogCDF(_actionLowerBounds[i], curMu, curSigma);
        logpOldPolicy += normalLogCDF(_actionLowerBounds[i], oldMu, oldSigma);
      }
      else if (_actionUpperBounds[i] <= action[i])
      {
        logpCurPolicy += normalLogCCDF(_actionUpperBounds[i], curMu, curSigma);
        logpOldPolicy += normalLogCCDF(_actionUpperBounds[i], oldMu, oldSigma);
      }
      else
      {
        logpCurPolicy += normalLogDensity(action[i], curMu, curSigma);
        logpOldPolicy += normalLogDensity(action[i], oldMu, oldSigma);
      }
    }
  }

  if (_policyDistribution == "Truncated Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float oldInvSig = 1.f / oldSigma;
      const float curInvSig = 1.f / curSigma;

      const float oldAlpha = (_actionLowerBounds[i] - oldMu) * oldInvSig * M_SQRT1_2;
      const float oldBeta = (_actionUpperBounds[i] - oldMu) * oldInvSig * M_SQRT1_2;

      const float curAlpha = (_actionLowerBounds[i] - curMu) * curInvSig * M_SQRT1_2;
      const float curBeta = (_actionUpperBounds[i] - curMu) * curInvSig * M_SQRT1_2;

      // log of normalization constants
      const float lCq = M_LN2 - safeLogMinus(gsl_sf_log_erfc(-curBeta), gsl_sf_log_erfc(-curAlpha));
      const float lCp = M_LN2 - safeLogMinus(gsl_sf_log_erfc(-oldBeta), gsl_sf_log_erfc(-oldAlpha));

      logpCurPolicy += lCq - std::log(curSigma) - 0.5 * (action[i] - curMu) * (action[i] - curMu) * curInvSig * curInvSig;
      logpOldPolicy += lCp - std::log(oldSigma) - 0.5 * (action[i] - oldMu) * (action[i] - oldMu) * oldInvSig * oldInvSig;
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldVariance = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      logpCurPolicy += betaLogDensityAlt(action[i], curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
      logpOldPolicy += betaLogDensityAlt(action[i], oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
    }
  }

  // Calculating log importance weight
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;

  // Normalizing extreme values to prevent loss of precision
  if (logImportanceWeight > +7.f) logImportanceWeight = +7.f;
  if (logImportanceWeight < -7.f) logImportanceWeight = -7.f;
  if (std::isfinite(logImportanceWeight) == false) KORALI_LOG_ERROR("NaN detected in the calculation of importance weight.\n");

  // Calculating actual importance weight by exp
  const float importanceWeight = std::exp(logImportanceWeight); // TODO: reuse importance weight calculation from updateExperienceReplayMetadata

  return importanceWeight;
}

std::vector<float> __className__::calculateImportanceWeightGradient(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  // Storage for importance weight gradients
  std::vector<float> importanceWeightGradients(_policyParameterCount, 0.);

  if (_policyDistribution == "Normal")
  {
    float logpCurPolicy = 0.f;
    float logpOldPolicy = 0.f;

    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      // Deviation from expAction and current Mean
      const float curActionDif = action[i] - curMean;

      // Inverse Variances
      const float curInvVar = 1.f / (curSigma * curSigma);

      // Gradient with respect to Mean
      importanceWeightGradients[i] = curActionDif * curInvVar;

      // Gradient with respect to Sigma
      importanceWeightGradients[_problem->_actionVectorSize + i] = (curActionDif * curActionDif) * (curInvVar / curSigma) - 1.f / curSigma;

      // Calculate importance weight
      logpCurPolicy += normalLogDensity(action[i], curMean, curSigma);
      logpOldPolicy += normalLogDensity(action[i], oldMean, oldSigma);
    }

    const float logImportanceWeight = logpCurPolicy - logpOldPolicy;
    const float importanceWeight = std::exp(logImportanceWeight); // TODO: reuse importance weight calculation from updateExperienceReplayMetadata

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++) importanceWeightGradients[i] *= importanceWeight;
  }

  if (_policyDistribution == "Squashed Normal")
  {
    float logpCurPolicy = 0.f;
    float logpOldPolicy = 0.f;

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float unboundedAction = oldPolicy.unboundedAction[i];

      // Deviation from expAction and current Mean
      const float curActionDif = unboundedAction - curMu;

      // Inverse Variance
      const float curInvVar = 1. / (curSigma * curSigma);

      // Gradient with respect to Mean
      importanceWeightGradients[i] = curActionDif * curInvVar;

      // Gradient with respect to Sigma
      importanceWeightGradients[_problem->_actionVectorSize + i] = (curActionDif * curActionDif) * (curInvVar / curSigma) - 1.0f / curSigma;

      // Importance weight of squashed normal is the importance weight of normal evaluated at unbounded action
      logpCurPolicy += normalLogDensity(unboundedAction, curMu, curSigma);
      logpOldPolicy += normalLogDensity(unboundedAction, oldMu, oldSigma);
    }

    const float logImportanceWeight = logpCurPolicy - logpOldPolicy;
    const float importanceWeight = std::exp(logImportanceWeight); // TODO: reuse importance weight calculation from updateExperienceReplayMetadata

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
      importanceWeightGradients[i] *= importanceWeight;
  }

  if (_policyDistribution == "Clipped Normal")
  {
    float logpCurPolicy = 0.f;
    float logpOldPolicy = 0.f;

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float curInvSig = 1.f / curSigma;

      // Deviation from expAction and current Mu
      const float curActionDif = (action[i] - curMu);

      if (action[i] <= _actionLowerBounds[i])
      {
        const float curNormalLogPdfLower = normalLogDensity(_actionLowerBounds[i], curMu, curSigma);
        const float curNormalLogCdfLower = normalLogCDF(_actionLowerBounds[i], curMu, curSigma);
        const float pdfCdfRatio = std::exp(curNormalLogPdfLower - curNormalLogCdfLower);

        // Grad wrt. curMu
        importanceWeightGradients[i] = -pdfCdfRatio;

        // Grad wrt. curSigma
        importanceWeightGradients[_problem->_actionVectorSize + i] = -curActionDif * curInvSig * pdfCdfRatio;

        // Calculate importance weight
        logpCurPolicy += curNormalLogCdfLower;
        logpOldPolicy += normalLogCDF(_actionLowerBounds[i], oldMu, oldSigma);
      }
      else if (_actionUpperBounds[i] <= action[i])
      {
        const float curNormalLogPdfUpper = normalLogDensity(_actionUpperBounds[i], curMu, curSigma);
        const float curNormalLogCCdfUpper = normalLogCCDF(_actionUpperBounds[i], curMu, curSigma);
        const float pdfCCdfRatio = std::exp(curNormalLogPdfUpper - curNormalLogCCdfUpper);

        // Grad wrt. curMu
        importanceWeightGradients[i] = pdfCCdfRatio;

        // Grad wrt. curSigma
        importanceWeightGradients[_problem->_actionVectorSize + i] = curActionDif * curInvSig * pdfCCdfRatio;

        // Calculate importance weight
        logpCurPolicy += curNormalLogCCdfUpper;
        logpOldPolicy += normalLogCCDF(_actionUpperBounds[i], oldMu, oldSigma);
      }
      else
      {
        // Inverse Variance
        const float curInvSig3 = curInvSig * curInvSig * curInvSig;

        // Grad wrt. curMu
        importanceWeightGradients[i] = curActionDif * curInvSig * curInvSig;

        // Grad wrt. curSigma
        importanceWeightGradients[_problem->_actionVectorSize + i] = curActionDif * curActionDif * curInvSig3 - curInvSig;

        // Calculate importance weight
        logpCurPolicy += normalLogDensity(action[i], curMu, curSigma);
        logpOldPolicy += normalLogDensity(action[i], oldMu, oldSigma);
      }
    }

    const float logImportanceWeight = logpCurPolicy - logpOldPolicy;
    const float importanceWeight = std::exp(logImportanceWeight); // TODO: reuse importance weight calculation from updateExperienceReplayMetadata

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < _policyParameterCount; i++)
      importanceWeightGradients[i] *= importanceWeight;
  }

  if (_policyDistribution == "Truncated Normal")
  {
    float logpCurPolicy = 0.f;
    float logpOldPolicy = 0.f;

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float oldInvSig = 1. / oldSigma;
      const float oldInvVar = oldInvSig * oldInvSig;

      const float curInvSig = 1. / curSigma;
      const float curInvVar = curInvSig * curInvSig;

      // Action differences to mu
      const float curActionDif = action[i] - curMu;
      const float oldActionDif = action[i] - oldMu;

      // Scaled upper and lower bound distances from mu
      const float oldAlpha = (_actionLowerBounds[i] - oldMu) * oldInvSig * M_SQRT1_2;
      const float oldBeta = (_actionUpperBounds[i] - oldMu) * oldInvSig * M_SQRT1_2;

      const float curAlpha = (_actionLowerBounds[i] - curMu) * curInvSig * M_SQRT1_2;
      const float curBeta = (_actionUpperBounds[i] - curMu) * curInvSig * M_SQRT1_2;

      // log of normalization constantsa
      const float lCq = M_LN2 - safeLogMinus(gsl_sf_log_erfc(-curBeta), gsl_sf_log_erfc(-curAlpha));
      const float lCp = M_LN2 - safeLogMinus(gsl_sf_log_erfc(-oldBeta), gsl_sf_log_erfc(-oldAlpha));

      // precomputing log values
      const float lPi2 = 0.5 * std::log(2. * M_PI);
      const float lCurSig = std::log(curSigma);
      const float lOldSig = std::log(oldSigma);

      // log of normalized gradients of normalization constants
      float ldCqMu = lCq - lPi2 - lCurSig;
      float dCqMu;

      const float curBeta2 = curBeta * curBeta;
      const float curAlpha2 = curAlpha * curAlpha;
      const float eps = 1e-7;

      if (definitelyLessThan(-curBeta2, -curAlpha2, eps))
      {
        ldCqMu += safeLogMinus(-curAlpha2, -curBeta2);
        dCqMu = -std::exp(ldCqMu);
      }
      else if (definitelyLessThan(-curAlpha2, -curBeta2, eps))
      {
        ldCqMu += safeLogMinus(-curBeta2, -curAlpha2);
        dCqMu = std::exp(ldCqMu);
      }
      else
      {
        ldCqMu = -100;
        dCqMu = 0.;
      }

      float dCqSig = std::exp(lCq - lPi2 - 2. * lCurSig - curAlpha2) * (curMu - _actionLowerBounds[i]) + std::exp(lCq - lPi2 - 2. * lCurSig - curBeta2) * (_actionUpperBounds[i] - curMu);

      // Gradient with respect to Mean
      importanceWeightGradients[i] = (curActionDif * curInvVar + dCqMu);
      assert(isfinite(importanceWeightGradients[i]));

      // Gradient with respect to Sigma
      importanceWeightGradients[_problem->_actionVectorSize + i] = curActionDif * curActionDif * curInvVar * curInvSig - curInvSig + dCqSig;
      assert(isfinite(importanceWeightGradients[_problem->_actionVectorSize + i]));

      // Calculate Importance Weight
      logpCurPolicy += lCq - lCurSig - 0.5 * curActionDif * curActionDif * curInvVar;
      logpOldPolicy += lCp - lOldSig - 0.5 * oldActionDif * oldActionDif * oldInvVar;
    }

    const float logImportanceWeight = logpCurPolicy - logpOldPolicy;
    const float importanceWeight = std::exp(logImportanceWeight); // TODO: reuse importance weight calculation from updateExperienceReplayMetadata

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
    {
      importanceWeightGradients[i] *= importanceWeight;
      assert(isfinite(importanceWeightGradients[i]));
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldVariance = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      float alphaCur;
      float betaCur;
      std::tie(alphaCur, betaCur) = betaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      float alphaOld;
      float betaOld;
      std::tie(alphaOld, betaOld) = betaParamTransformAlt(oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Log probability of action with old policy params
      const float logpOldPolicy = betaLogDensityAlt(action[i], oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
      const float invpOldPolicy = std::exp(-logpOldPolicy);

      // Variable preparation
      const float Bab = gsl_sf_beta(alphaCur, betaCur);

      const float psiAb = gsl_sf_psi(alphaCur + betaCur);

      const float actionRange = _actionUpperBounds[i] - _actionLowerBounds[i];
      const float logscale = std::log(actionRange);
      const float powscale = std::pow(actionRange, -betaCur - alphaCur + 1.f);
      const float factor = -1.f * std::pow(action[i] - _actionLowerBounds[i], alphaCur - 1.f) * powscale * std::pow(_actionUpperBounds[i] - action[i], betaCur - 1.f) * invpOldPolicy / Bab;

      // Rho Grad wrt alpha and beta
      const float daBab = gsl_sf_psi(alphaCur) - psiAb;
      const float drhoda = ((logscale - std::log(action[i] - _actionLowerBounds[i])) + daBab) * factor;
      const float dbBab = gsl_sf_psi(betaCur) - psiAb;
      const float drhodb = (logscale - std::log(_actionUpperBounds[i] - action[i]) + dbBab) * factor;

      // Derivatives of alpha and beta wrt mu and varc
      float dadmu, dadvarc, dbdmu, dbdvarc;
      std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Rho Grad wrt mu and varc
      importanceWeightGradients[i] = drhoda * dadmu + drhodb * dbdmu;
      importanceWeightGradients[_problem->_actionVectorSize + i] = drhoda * dadvarc + drhodb * dbdvarc;
    }
  }

  return importanceWeightGradients;
}

std::vector<float> __className__::calculateKLDivergenceGradient(const policy_t &oldPolicy, const policy_t &curPolicy)
{
  // Storage for KL Divergence Gradients
  std::vector<float> KLDivergenceGradients(_policyParameterCount, 0.0);

  if (_policyDistribution == "Normal" || _policyDistribution == "Squashed Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float curInvSig = 1. / curSigma;
      const float curInvVar = 1. / (curSigma * curSigma);
      const float curInvSig3 = 1. / (curSigma * curSigma * curSigma);
      const float actionDiff = (curMean - oldMean);

      // KL-Gradient with respect to Mean
      KLDivergenceGradients[i] = actionDiff * curInvVar;

      // Contribution to Sigma from Trace
      const float gradTr = -curInvSig3 * oldSigma * oldSigma;

      // Contribution to Sigma from Quadratic term
      const float gradQuad = -(actionDiff * actionDiff) * curInvSig3;

      // Contribution to Sigma from Determinant
      const float gradDet = curInvSig;

      // KL-Gradient with respect to Sigma
      KLDivergenceGradients[_problem->_actionVectorSize + i] = gradTr + gradQuad + gradDet;
    }
  }

  if (_policyDistribution == "Clipped Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      // Precompute often used constant terms
      const float oldVar = oldSigma * oldSigma;
      const float oldInvSig = 1.f / oldSigma;
      const float curInvSig = 1.f / curSigma;
      const float curInvVar = 1.f / (curSigma * curSigma);
      const float curInvSig3 = 1.f / (curSigma * curSigma * curSigma);
      const float muDif = (oldMu - curMu);

      const float invSqrt2Pi = M_SQRT1_2 * std::sqrt(M_1_PI);

      const float oldAdjustedLb = (_actionLowerBounds[i] - oldMu) * oldInvSig;
      const float oldAdjustedUb = (_actionUpperBounds[i] - oldMu) * oldInvSig;

      const float curAdjustedLb = (_actionLowerBounds[i] - curMu) * curInvSig;
      const float curAdjustedUb = (_actionUpperBounds[i] - curMu) * curInvSig;

      const float erfLb = std::erf(M_SQRT1_2 * oldAdjustedLb);
      const float erfUb = std::erf(M_SQRT1_2 * oldAdjustedUb);

      const float expLb = std::exp(-0.5f * oldAdjustedLb * oldAdjustedLb);
      const float expUb = std::exp(-0.5f * oldAdjustedUb * oldAdjustedUb);

      const float cdfRatiosA = std::exp(normalLogCDF(_actionLowerBounds[i], oldMu, oldSigma) + normalLogDensity(_actionLowerBounds[i], curMu, curSigma) - normalLogCDF(_actionLowerBounds[i], curMu, curSigma));
      const float ccdfRatiosB = std::exp(normalLogCCDF(_actionUpperBounds[i], oldMu, oldSigma) + normalLogDensity(_actionUpperBounds[i], curMu, curSigma) - normalLogCCDF(_actionUpperBounds[i], curMu, curSigma));

      // KL-Gradient with respect to Mean
      KLDivergenceGradients[i] = cdfRatiosA;
      KLDivergenceGradients[i] -= 0.5f * muDif * curInvVar * (erfUb - erfLb);
      KLDivergenceGradients[i] += invSqrt2Pi * oldSigma * curInvVar * (expUb - expLb);
      KLDivergenceGradients[i] -= ccdfRatiosB;

      // KL-Gradient with respect to Sigma
      KLDivergenceGradients[_problem->_actionVectorSize + i] = curAdjustedLb * cdfRatiosA;
      KLDivergenceGradients[_problem->_actionVectorSize + i] += 0.5f * (curInvSig - muDif * muDif * curInvSig3 - oldVar * curInvSig3) * (erfUb - erfLb);
      KLDivergenceGradients[_problem->_actionVectorSize + i] += invSqrt2Pi * curInvSig3 * (oldVar * oldAdjustedUb + 2.f * oldSigma * muDif) * expUb;
      KLDivergenceGradients[_problem->_actionVectorSize + i] -= invSqrt2Pi * curInvSig3 * (oldVar * oldAdjustedLb + 2.f * oldSigma * muDif) * expLb;
      KLDivergenceGradients[_problem->_actionVectorSize + i] -= curAdjustedUb * ccdfRatiosB;
    }
  }

  if (_policyDistribution == "Truncated Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      // Precompute often used constant terms
      const float oldVar = oldSigma * oldSigma;
      const float oldInvSig = 1.f / oldSigma;

      const float curInvSig = 1.f / curSigma;
      const float curInvVar = curInvSig * curInvSig;
      const float curInvSig3 = curInvSig * curInvVar;
      const float muDif = (oldMu - curMu);

      // old scaled upper and lower bound distances from mu
      const float oldAlpha = (_actionLowerBounds[i] - oldMu) * oldInvSig * M_SQRT1_2;
      const float oldBeta = (_actionUpperBounds[i] - oldMu) * oldInvSig * M_SQRT1_2;

      // current scaled upper and lower bound distances from mu
      const float curAlpha = (_actionLowerBounds[i] - curMu) * curInvSig * M_SQRT1_2;
      const float curBeta = (_actionUpperBounds[i] - curMu) * curInvSig * M_SQRT1_2;

      // log of normalization constantsa
      const float lCq = M_LN2 - safeLogMinus(gsl_sf_log_erfc(-curBeta), gsl_sf_log_erfc(-curAlpha));
      const float lCp = M_LN2 - safeLogMinus(gsl_sf_log_erfc(-oldBeta), gsl_sf_log_erfc(-oldAlpha));

      // precomputing log values
      const float lPi2 = 0.5 * std::log(2. * M_PI);
      const float lCurSig = std::log(curSigma);
      const float lOldSig = std::log(oldSigma);

      // log of normalized gradients of normalization constants
      float ldCqMu = lCq - lPi2 - lCurSig;
      float dCqMu;

      float lCps = lCp - 0.5 * lPi2 + lOldSig - lCurSig;
      float Cps;

      const float curBeta2 = curBeta * curBeta;
      const float curAlpha2 = curAlpha * curAlpha;
      const float eps = 1e-6;

      if (definitelyLessThan(-curBeta2, -curAlpha2, eps))
      {
        const float logDif = safeLogMinus(-curAlpha2, -curBeta2);
        ldCqMu += logDif;
        dCqMu = -std::exp(ldCqMu);
        lCps += logDif;
        Cps = -std::exp(lCps);
      }
      else if (definitelyLessThan(-curAlpha2, -curBeta2, eps))
      {
        const float logDif = safeLogMinus(-curBeta2, -curAlpha2);
        ldCqMu += logDif;
        dCqMu = std::exp(ldCqMu);
        lCps += logDif;
        Cps = std::exp(lCps);
      }
      else
      {
        ldCqMu = -100;
        dCqMu = 0.;
        Cps = 0.;
      }

      float dCqSig = std::exp(lCq - lPi2 - 2. * lCurSig - curAlpha2) * (curMu - _actionLowerBounds[i]) + std::exp(lCq - lPi2 - 2. * lCurSig - curBeta2) * (_actionUpperBounds[i] - curMu);

      // KL-Gradient with respect to Mu
      KLDivergenceGradients[i] = -dCqMu - muDif * curInvVar + Cps;

      // Precompute some terms
      const float sb = oldSigma * curInvSig3 * (_actionUpperBounds[i] + oldMu - 2. * curMu);
      const float sa = oldSigma * curInvSig3 * (_actionLowerBounds[i] + oldMu - 2. * curMu);

      const float lCpb = lCp - oldBeta * oldBeta - lPi2;
      const float lCpa = lCp - oldAlpha * oldAlpha - lPi2;

      const float Cpb = std::exp(lCpb);
      const float Cpa = std::exp(lCpa);

      // KL-Gradient with respect to Sigma
      KLDivergenceGradients[_problem->_actionVectorSize + i] = -muDif * muDif * curInvSig3 + curInvSig - dCqSig - oldVar * curInvSig3 + Cpb * sb - Cpa * sa;
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldVariance = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      float alphaCur;
      float betaCur;
      std::tie(alphaCur, betaCur) = betaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      float alphaOld;
      float betaOld;
      std::tie(alphaOld, betaOld) = betaParamTransformAlt(oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Constants involving psi function
      const float psiAbCur = gsl_sf_psi(alphaCur + betaCur);
      const float psiAbOld = gsl_sf_psi(alphaOld + betaOld);

      const float actionRange = _actionUpperBounds[i] - _actionLowerBounds[i];

      // KL Grad wrt alpha
      const float dklda = (gsl_sf_psi(alphaCur) - psiAbCur - gsl_sf_psi(alphaOld) - psiAbOld) / actionRange;

      // KL Grad wrt beta
      const float dkldb = (gsl_sf_psi(betaCur) - psiAbCur - gsl_sf_psi(betaOld) - psiAbOld) / actionRange;

      // Derivatives of alpha and beta wrt mu and varc
      float dadmu, dadvarc, dbdmu, dbdvarc;
      std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // KL Grad wrt mu and varc
      KLDivergenceGradients[i] = dklda * dadmu + dkldb * dbdmu;
      KLDivergenceGradients[_problem->_actionVectorSize + i] = dklda * dadvarc + dkldb * dbdvarc;
    }
  }

  return KLDivergenceGradients;
}

float __className__::evaluateTrajectoryLogProbability(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions, const std::vector<float> &policyHyperparameter)
{
  knlohmann::json policy;
  policy["Policy"] = policyHyperparameter;
  setPolicy(policy);

  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    auto evaluation = runPolicy({{states[t]}})[0];
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation.distributionParameters[d], evaluation.distributionParameters[_problem->_actionVectorSize + d]);
  }

  return trajectoryLogProbability;
}

float __className__::evaluateTrajectoryLogProbabilityWithObservedPolicy(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions)
{
  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    std::vector<float> evaluation(_problem->_actionVectorSize);
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
    {
      // Predict action with linear policy
      evaluation[d] = _observationsApproximatorWeights[d][0];
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        evaluation[d] += states[t][j] * _observationsApproximatorWeights[d][j + 1];
    }
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation[d], _observationsApproximatorSigmas[d]);
  }

  return trajectoryLogProbability;
}

__moduleAutoCode__;

__endNamespace__;
