{

  "Module Data":
  {
    "Class Name": "Agent",
    "Namespace": ["korali", "solver"],
    "Parent Class Name": "Solver"
  },

 "Configuration Settings":
 [
   {
   "Name": [ "Mode" ],
   "Type": "std::string",
   "Options": [
               { "Value": "Training", "Description": "Learns a policy for the reinforcement learning problem." },
               { "Value": "Testing", "Description": "Tests the policy with a learned policy." }
              ],
   "Description": "Specifies the operation mode for the agent."
  },
  {
    "Name": [ "Testing", "Sample Ids" ],
    "Type": "std::vector<size_t>",
    "Description": "A vector with the identifiers for the samples to test the hyperparameters with."
  },
  {
    "Name": [ "Testing", "Policy" ],
    "Type": "knlohmann::json",
    "Description": "The hyperparameters of the policy to test."
  },
  {
    "Name": [ "Training", "Average Depth" ],
    "Type": "size_t",
    "Description": "Specifies the depth of the running training average to report."
  },
  {
    "Name": [ "Agent Count" ],
    "Type": "size_t",
    "Description": "Indicates the number of concurrent agents collecting experiences."
  },
  {
    "Name": [ "Episodes Per Generation" ],
    "Type": "size_t",
    "Description": "Indicates the how many finished episodes to receive in a generation (checkpoints are generated between generations)."
  },
  {
    "Name": [ "Mini Batch", "Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
   "Name": [ "Mini Batch", "Strategy" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Uniform", "Description": "Selects experiences from the replay memory with a random uniform probability distribution." }
     ],
   "Description": "Determines how to select experiences from the replay memory for mini batch creation."
  },
  {
    "Name": [ "Time Sequence Length" ],
    "Type": "size_t",
    "Description": "Indicates the number of contiguous experiences to pass to the NN for learning. This is only useful when using recurrent NNs on problems with time-dependent phenomena."
  },
  {
    "Name": [ "Learning Rate" ],
    "Type": "float",
    "Description": "The base learning rate to use for the NN hyperparameter optimization."
  },
  {
   "Name": [ "L2 Regularization", "Enabled" ],
   "Type": "bool",
   "Description": "Regulates if l2 regularization will be applied to the neural networks."
  },
  {
   "Name": [ "L2 Regularization", "Importance" ],
   "Type": "float",
   "Description": "Importance weight of l2 regularization."
  },
  {
    "Name": [ "Neural Network", "Hidden Layers" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the hidden neural network layers."
  },
  {
    "Name": [ "Neural Network", "Optimizer" ],
    "Type": "std::string",
    "Description": "Indicates the optimizer algorithm to use for the NN hyperparameters."
  },
  {
   "Name": [ "Neural Network", "Engine" ],
   "Type": "std::string",
   "Description": "Specifies which Neural Network backend engine to use."
  },
  {
   "Name": [ "Discount Factor" ],
   "Type": "float",
   "Description": "Represents the weight given to the expectation of the cumulative reward from future experiences."
  },
  {
    "Name": [ "Experience Replay", "Serialize" ],
    "Type": "bool",
    "Description": "Indicates whether to serialize and store the experience replay after each generation file save. Disabling will reduce I/O overheads but will disable the checkpoint/resume function."
  },
  {
    "Name": [ "Experience Replay", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Experience Replay", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Cutoff Scale" ],
    "Type": "float",
    "Description": "Indicates the scaling of the importance weight threshold under/above which an experience is considered off-policy. "
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Target" ],
    "Type": "float",
    "Description": "Indicates the target off-policiness. The agent will try to stay under this target value"
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Annealing Rate" ],
    "Type": "float",
    "Description": "The parameters A from eq. (13) for REFER  (see: https://arxiv.org/abs/1807.05827) for controling experience off-policiness in the ER memory."
  },
  {
   "Name": [ "Experience Replay", "Off Policy", "REFER Beta" ],
   "Type": "float",
   "Description": "Initial value for the penalisation coefficient for off-policiness. "
  },
  {
    "Name": [ "Experiences Between Policy Updates" ],
    "Type": "float",
    "Description": "The number of experiences to receive before training/updating (real number, may be less than < 1.0, for more than one update per experience)."
  },
  {
    "Name": [ "State Rescaling", "Enabled" ],
    "Type": "bool",
    "Description": "Determines whether to use state scaling (done only once after the initial exploration phase)."
  },
  {
    "Name": [ "Reward", "Rescaling", "Enabled" ],
    "Type": "bool",
    "Description": "Determines whether to use reward scaling"
  },
  {
    "Name": [ "Reward", "Rescaling", "Frequency" ],
    "Type": "size_t",
    "Description": "The number of policy updates between consecutive reward rescalings."
  },
  {
    "Name": [ "Reward", "Outbound Penalization", "Enabled" ],
    "Type": "bool",
    "Description": "If enabled, it penalizes the rewards for experiences that contain out of bound actions. This is useful for problems with truncated actions (e.g., openAI gym Mujoco) where out of bounds actions produce the same effect as the boundary action to prevent policy means to extend too much outside the bounds."
  },
  {
    "Name": [ "Reward", "Outbound Penalization", "Factor" ],
    "Type": "float",
    "Description": "The factor (f) by which te reward is scaled down. R = f * R"
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxExperiences > 0) && (_experienceCount >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Testing", "Target Average Reward" ],
    "Type": "float",
    "Criteria": "(_mode == \"Training\") && (_testingTargetAverageReward > -korali::Inf) && (_testingBestAverageReward >= _testingTargetAverageReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  },
  {
    "Name": [ "Testing", "Average Reward Increment" ],
    "Type": "float",
    "Criteria": "(_mode == \"Training\") && (_testingAverageRewardIncrement > 0.0) && (_testingPreviousAverageReward > -korali::Inf) && (_testingAverageReward + _testingStdevReward * _testingAverageRewardIncrement < _testingPreviousAverageReward)",
    "Description": "The solver will stop when the averge testing reward is below the previous testing average by more than a threshold given by this factor multiplied with the testing standard deviation."
  },
  {
    "Name": [ "Max Policy Updates" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxPolicyUpdates > 0) && (_policyUpdateCount >= _maxPolicyUpdates)",
    "Description": "The solver will stop when the given number of optimizations have been made to the learner."
  }
 ],

 "Variables Configuration":
 [

 ],

 "Internal Settings":
 [
  {
    "Name": [ "Action Lower Bounds" ],
    "Type": "std::vector<float>",
    "Description": "Lower bounds for actions."
  },
  {
    "Name": [ "Action Upper Bounds" ],
    "Type": "std::vector<float>",
    "Description": "Upper bounds for actions."
  },
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
   "Name": [ "Last Training Reward" ],
   "Type": "float",
   "Description": "The cumulative training reward for the last episode received."
  },
  {
    "Name": [ "Training", "Reward History" ],
    "Type": "std::vector<float>",
    "Description": "Keeps a history of all training episode rewards."
  },
  {
    "Name": [ "Training", "Experience History" ],
    "Type": "std::vector<size_t>",
    "Description": "Keeps a history of all training episode experience counts."
  },
  {
    "Name": [ "Training", "Average Reward" ],
    "Type": "float",
    "Description": "Contains a running average of the training episode rewards."
  },
  {
    "Name": [ "Training", "Last Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative reward of the last training episode."
  },
  {
    "Name": [ "Training", "Best Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative average episode reward found so far from training episodes."
  },
  {
    "Name": [ "Training", "Best Episode Id" ],
    "Type": "size_t",
    "Description": "Remembers the episode that obtained the maximum reward found so far during training."
  },
  {
    "Name": [ "Testing", "Reward" ],
    "Type": "std::vector<float>",
    "Description": "The rewards obtained when evaluating the testing samples."
  },
  {
    "Name": [ "Testing", "Best Reward" ],
    "Type": "float",
    "Description": "Remembers the best reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Worst Reward" ],
    "Type": "float",
    "Description": "Remembers the worst reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Best Episode Id" ],
    "Type": "size_t",
    "Description": "Remembers the episode Id that obtained the maximum reward found so far during testing."
  },
  {
    "Name": [ "Testing", "Candidate Count" ],
    "Type": "size_t",
    "Description": "Remembers the number of candidate policies tested so far."
  },
  {
    "Name": [ "Testing", "Average Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Stdev Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Previous Average Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from previous testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Best Average Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative average episode reward found so far from testing episodes."
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Count" ],
    "Type": "size_t",
    "Description": "Number of current off-policy experiences in the ER."
  }, 
  {
    "Name": [ "Experience Replay", "Off Policy", "Ratio" ],
    "Type": "float",
    "Description": "Current off policy ratio in the experience replay."
  }, 
  {
    "Name": [ "Experience Replay", "Off Policy", "Current Cutoff" ],
    "Type": "float",
    "Description": "Indicates the current importance weight threshold under which an experience is considered off-policy. "
  },
  {
    "Name": [ "Current Learning Rate" ],
    "Type": "float",
    "Description": "The current learning rate to use for the NN hyperparameter optimization."
  },
  {
   "Name": [ "Policy Update Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of policy updates performed."
  },
  {
   "Name": [ "Current Sample ID" ],
   "Type": "size_t",
   "Description": "Keeps track of the current Sample ID, to make sure no two equal sample IDs are produced and that this value can be used as random seed."
  },
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator."
  },
  {
    "Name": [ "Experience Count" ],
    "Type": "size_t",
    "Description": "Takes count of the number of experiences received so far."
  },
  {
    "Name": [ "Reward", "Rescaling", "Mean" ],
    "Type": "float",
    "Description": "Contains the mean by which rewards are rescaled to a N[0,1] distribution."
  },
  {
    "Name": [ "Reward", "Rescaling", "Sigma" ],
    "Type": "float",
    "Description": "Contains the sigma by which rewards are rescaled to a N[0,1] distribution."
  },
  {
    "Name": ["Reward", "Rescaling", "Count" ],
    "Type": "size_t",
    "Description": "Indicates how many times have the rewards been rescaled"
  },
  {
    "Name": [ "Reward", "Outbound Penalization", "Count" ],
    "Type": "size_t",
    "Description": "Keeps track of the number of out of bound actions taken."
  },
  {
    "Name": [ "State Rescaling", "Means" ],
    "Type": "std::vector<float>",
    "Description": "Contains the means by which state variables are rescaled to a N[0,1] distribution."
  },
  {
    "Name": [ "State Rescaling", "Sigmas" ],
    "Type": "std::vector<float>",
    "Description": "Contains the sigmas by which state variables are rescaled to a N[0,1] distribution."
  }
  
 ],

 "Module Defaults":
 {
   "Episodes Per Generation": 1,
   "Agent Count": 1,
   "Discount Factor": 0.995,
   "Time Sequence Length": 1,
   
   "State Rescaling": 
   {
    "Enabled": false
   },
   
   "Reward":
   {
    "Rescaling": 
    {
     "Enabled": false,
     "Frequency": 1000
    },
    
    "Outbound Penalization":
    {
     "Enabled": false,
     "Factor": 0.5
    }
   },

   "Mini Batch":
    {
     "Strategy": "Uniform",
     "Size": 256
    },
       
   "L2 Regularization": 
   {
     "Enabled": false,
     "Importance": 1e-4
   },
   
   "Training":
   {
    "Average Depth": 100
   },

   "Testing":
   {
    "Sample Ids": [ ],
    "Policy": { }
   },

   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Policy Updates": 0,
    "Testing":
    {
     "Target Average Reward": -Infinity,
     "Average Reward Increment": 0.0
    }
   },

  "Experience Replay":
   {
    "Serialize": true,
    "Start Size": 0,
    "Maximum Size": 0,
    
    "Off Policy":
    {
     "Cutoff Scale": -1.0,
     "Target": 0.1,
     "REFER Beta": 0.3,
     "Annealing Rate": 0.0
    }
   },

   "Uniform Generator":
   {
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   }
 },

 "Variable Defaults":
 {

 }

}
