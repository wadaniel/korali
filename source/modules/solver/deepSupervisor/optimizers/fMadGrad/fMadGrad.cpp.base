#include "modules/solver/deepSupervisor/optimizers/fMadGrad/fMadGrad.hpp"

__startNamespace__;

void __className__::initialize()
{
  fGradientBasedOptimizer::initialize();
  _initialValue.resize(_nVars, 0.0f);
  _s.resize(_nVars, 0.0f);
  _v.resize(_nVars, 0.0f);
  _z.resize(_nVars, 0.0f);
  reset();
}

void __className__::reset()
{
#pragma omp parallel for simd
  for (size_t i = 0; i < _nVars; i++)
  {
    _currentValue[i] = 0.0f;
    _initialValue[i] = 0.0f;
    _s[i] = 0.0f;
    _v[i] = 0.0f;
    _z[i] = 0.0f;
  }
}

void __className__::processResult(std::vector<float> &gradient)
{
  __parentClassName__::preProcessResult(gradient);

  float lambda = _eta; // * std::sqrt((float)_modelEvaluationCount + 1.0f);
#pragma omp parallel for simd
  for (size_t i = 0; i < _nVars; i++)
  {
    _s[i] = _s[i] + lambda * gradient[i];
    _v[i] = _v[i] - lambda * (gradient[i] * gradient[i]);
    _z[i] = _initialValue[i] - (1.0f / (std::cbrt(_v[i]) + _epsilon)) * _s[i];
    _currentValue[i] = (1.0f - _momentum) * _currentValue[i] + _momentum * _z[i];
  }

  __parentClassName__::postProcessResult(_currentValue);
}

void __className__::printInternals()
{
  printf("_currentValue[i], _s[i], _v[i], _z[i]:\n");
  for (size_t i = 0; i < _nVars; i++)
    printf("%f %f %f %f\n", _currentValue[i], _s[i], _v[i], _z[i]);
}

__moduleAutoCode__;

__endNamespace__;
