#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <chrono>

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  //  Pre-allocating space for the experience replay memory
  _experienceReplay.resize(_experienceReplayMaximumSize);

  /*********************************************************************
   *   // If initial generation, set initial agent configuration
   *********************************************************************/

  if (_k->_currentGeneration == 0)
  {
    _randomActionProbabilityCurrentValue = _randomActionProbabilityInitialValue;
    _currentEpisode = 0;
    _policyUpdateCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;
    _cumulativeQStar = 0;
    _experienceCount = 0;
    _experienceReplayMaxPriority = 0.0;
    _randomActionProbabilityEffectiveValue = 1.0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTestingReward = -korali::Inf;
    _worstTestingReward = +korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestTrainingEpisode = 0;
    _bestAverageTestingReward = -korali::Inf;

    // Initialize best hyperparameters
    _bestHyperparameters = _hyperparameters;
  }

  // Initializing profiling timers
  _agentStartTime = 0.0;
  _agentWaitTime = 0.0;
}

void Agent::runGeneration()
{
  // Storage for agents
  std::vector<Sample> agents(_episodesPerGeneration);

  // Storage to keep experience information
  knlohmann::json experience;

  // Storage to keep track of finished agents
  std::vector<bool> agentHasFinished(_episodesPerGeneration, false);

  // Storage for the entire episodes before adding it to the replay memory
  std::vector<std::vector<knlohmann::json>> episodes(_episodesPerGeneration);

  // Configuring and launching agents
  for (size_t agentId = 0; agentId < _episodesPerGeneration; agentId++)
  {
   agents[agentId]["Sample Id"] = _currentSampleID++;
   agents[agentId]["Module"] = "Problem";
   agents[agentId]["Operation"] = "Run Episode";
   agents[agentId]["Hyperparameters"] = _hyperparameters;
   agents[agentId]["Random Action Probability"] = _randomActionProbabilityEffectiveValue;

   KORALI_START(agents[agentId]);
  }

  // Running until all agents have finished
  size_t finishedAgents = 0;
  while (finishedAgents < _episodesPerGeneration)
  {
   // Listening to agents for incoming experiences
   KORALI_LISTEN(agents);

   for (size_t agentId = 0; agentId < _episodesPerGeneration; agentId++)
    if (agentHasFinished[agentId] == false)
    if (agents[agentId].retrievePendingMessage(experience))
    {
      // Storage to determine terminal state
      bool isTerminal = false;

      // Storing experience in the episode
      episodes[agentId].push_back(experience);

      // Increasing total experience counter
      _experienceCount++;

      // Check if experience is terminal
      isTerminal = experience["Termination"] != "Non Terminal";

      // If agent requested new policy, send the new hyperparameters
      if (experience["Request New Policy"] == true)
        KORALI_SEND_MSG_TO_SAMPLE(agents[agentId], _hyperparameters);

      if (isTerminal)
      {
       // Now that we have the entire episode, process its experiences (add them to replay memory)
       for (size_t i = 0; i < episodes[agentId].size(); i++) processExperience(episodes[agentId][i]);

       // Waiting for the agent to come back with all the information
       KORALI_WAIT(agents[agentId]);

       // Getting the training reward of the latest episode
       _lastTrainingReward = agents[agentId]["Training Reward"].get<float>();

       // Keeping training statistics. Updating if exceeded best training policy so far.
       if (_lastTrainingReward > _bestTrainingReward)
       {
         _bestTrainingReward = _lastTrainingReward;
         _bestTrainingEpisode = _currentEpisode;
       }

       // If the policy has exceeded the threshold during training, we gather its statistics
       if (agents[agentId]["Tested Policy"] == true)
       {
        _candidatePoliciesTested++;

        _averageTestingReward = agents[agentId]["Average Testing Reward"].get<float>();
        _bestTestingReward = agents[agentId]["Best Testing Reward"].get<float>();
        _worstTestingReward = agents[agentId]["Worst Testing Reward"].get<float>();

        // If the average testing reward is better than the previous best, replace it
        // and store hyperparameters as best so far.
        if (_averageTestingReward > _bestAverageTestingReward)
        {
          _bestAverageTestingReward = _averageTestingReward;
          _bestHyperparameters = agents[agentId]["Hyperparameters"];
        }
       }

       // Set agent as finished
       agentHasFinished[agentId] = true;

       // Increase finished agent count
       finishedAgents++;

       // Increasing number of episodes
       _currentEpisode++;

       // Updating experience probabilities
       if (_miniBatchStrategy == "Prioritized") updateExperienceReplayProbabilities();
      }
    }

   // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
   if (_experienceReplay.size() >= _experienceReplayStartSize)
   {
    // Defining new experience count threshold
    size_t curThreshold = _experiencesBetweenPolicyUpdates*_policyUpdateCount  + _experienceReplayStartSize;

    // If we accumulated enough experiences between updates, update now
    if (_experienceCount > curThreshold) { trainPolicy();  _policyUpdateCount++; }
   }
  }
}

void Agent::processExperience(knlohmann::json &experience)
{
  experience_t e;

  e.cache.setMaxAge(_cachePersistence);
  e.cache.setTimer(&_policyUpdateCount);

  // Getting state, action, reward, and whether it's terminal
  e.state = experience["State"].get<std::vector<float>>();
  e.action = experience["Action"].get<std::vector<float>>();
  e.reward = experience["Reward"].get<float>();

  if (experience["Termination"] == "Non Terminal") e.termination = e_nonTerminal;
  if (experience["Termination"] == "Terminal") e.termination = e_terminal;
  if (experience["Termination"] == "Truncated")
  {
    e.termination = e_truncated;
    e.truncatedState = experience["Truncated State"].get<std::vector<float>>();
  }

  e.priority = _experienceReplayMaxPriority;
  e.probability = 0.0;
  e.policy = experience["Policy"];
  _experienceReplay.add(e);
}

void Agent::updateExperienceReplayProbabilities()
{
  // Rank priorities
  auto experienceRanking = std::vector<std::pair<float, size_t>>(_experienceReplay.size());
  for (size_t i = 0; i < _experienceReplay.size(); ++i) experienceRanking[i] = std::make_pair(_experienceReplay[i].priority, i + 1);
  sort(experienceRanking.begin(), experienceRanking.end());

  // Calculate rank based probabilities
  float prioritySum = 0.0;
  for (size_t i = 1; i < _experienceReplay.size() + 1; ++i)
    prioritySum += std::pow((float)i, _priorityAnnealingRate);

  // Update probabilities
  for (size_t i = 0; i < _experienceReplay.size(); ++i)
    _experienceReplay[i].probability = std::pow((float)experienceRanking[i].second, _priorityAnnealingRate) / (prioritySum);
}

void Agent::updateExperienceReplayPriority(size_t expId, float priority)
{
  // Insert priority calculated in learner
  if (priority > _experienceReplayMaxPriority) _experienceReplayMaxPriority = priority;
  _experienceReplay[expId].priority = priority;
}

std::vector<float> Agent::calculateImportanceWeights(std::vector<size_t> miniBatch)
{
  auto importanceWeights = std::vector<float>(miniBatch.size(), 1.0);

  if (_miniBatchStrategy == "Prioritized")
  {
    for (size_t i = 0; i < miniBatch.size(); ++i)
      importanceWeights[i] = std::pow((float)_experienceReplay.size() * _experienceReplay[miniBatch[i]].probability, -_importanceWeightAnnealingRate);
  }

  return importanceWeights;
}

std::vector<size_t> Agent::generateMiniBatch(size_t size)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(size);

  if (_miniBatchStrategy == "Uniform")
  {
    for (size_t i = 0; i < size; i++)
    {
      // Producing random (uniform) number for the selection of the experience
      float x = _uniformGenerator->getRandomNumber();

      // Selecting experience
      size_t expId = std::floor(x * (float)(_experienceReplay.size() - 1));

      // Setting experience
      miniBatch[i] = expId;
    }
  }

  if (_miniBatchStrategy == "Prioritized")
  {
    // Allocating storage for uniform random numbers
    auto pvalues = std::vector<float>(size);

    for (size_t i = 0; i < size; i++)
      pvalues[i] = _uniformGenerator->getRandomNumber();

    // Sort selections ascending for traversal
    std::sort(pvalues.begin(), pvalues.end());

    size_t expId = 0;
    size_t batchIdx = 0;
    float cumulativeProbability = 0;

    // Sampling from multinomial distribution using inverse transform
    cumulativeProbability += _experienceReplay[0].probability;
    while (batchIdx < size && expId < _experienceReplay.size())
    {
      if (cumulativeProbability > pvalues[batchIdx])
        miniBatch[batchIdx++] = expId;
      else
        cumulativeProbability += _experienceReplay[++expId].probability;
    }

    // Uniform fill if miniBatch not full due to numerical precision during probability accumulation (cumulative probability < 1)
    while (batchIdx < size)
      miniBatch[batchIdx++] = std::floor(_uniformGenerator->getRandomNumber() * (float)(_experienceReplay.size() - 1));
  }

  return miniBatch;
}

void Agent::updateRandomActionProbability()
{
  // If we reached the starting size
  if (_experienceReplay.size() > _experienceReplayStartSize)
  {
    // Decreasing the value of epsilon
    _randomActionProbabilityCurrentValue = _randomActionProbabilityCurrentValue - _randomActionProbabilityDecreaseRate;

    // Without exceeding the lower bound
    if (_randomActionProbabilityCurrentValue < _randomActionProbabilityTargetValue) _randomActionProbabilityCurrentValue = _randomActionProbabilityTargetValue;
  }

  // Calculating effective value for random action probability
  _randomActionProbabilityEffectiveValue = _experienceReplay.size() > _experienceReplayStartSize ? _randomActionProbabilityCurrentValue : 1.0;
}

void Agent::finalize()
{
  // Updating policy with the best parameters produced during training
  updateAgentPolicy(_bestHyperparameters);
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplay.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  if (_maxPolicyUpdates > 0)
    _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu/%lu\n", _policyUpdateCount, _maxPolicyUpdates);
  else
    _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu\n", _policyUpdateCount);

  _k->_logger->logInfo("Normal", " + Latest Reward:              %f/%f\n", _lastTrainingReward, _problem->_trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f (%lu)\n", _bestTrainingReward, _bestTrainingEpisode);
  _k->_logger->logInfo("Normal", " + Current p(Random Action):       %f\n", _randomActionProbabilityEffectiveValue);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average (Worst / Best) Reward: %f (%f / %f)\n", _averageTestingReward, _worstTestingReward, _bestTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward: %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward: %f\n", _bestAverageTestingReward);

  printAgentInformation();

  _k->_logger->logInfo("Normal", "Profiling Information:\n");
  _k->_logger->logInfo("Normal", " + Agent Start Time:            %fs\n", _agentStartTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + Agent Wait Time:             %fs\n", _agentWaitTime / 1.0e+9f);
}

} // namespace solver

} // namespace korali
